{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "7d1fb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/kegarcia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, enum, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "327878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_POSTAGS = ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']\n",
    "NLP_MODELS = {\n",
    "    'en_core_web_trf': spacy.load('en_core_web_trf'),    \n",
    "    'en_core_web_sm': spacy.load('en_core_web_sm'),\n",
    "    'en_core_web_md': spacy.load('en_core_web_md'),\n",
    "    'en_core_web_lg': spacy.load('en_core_web_lg')\n",
    "}\n",
    "\n",
    "class Sentiments(enum.Enum):\n",
    "    POS = 'POS'\n",
    "    NEG = 'NEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "bbc8b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # not necesarry but just as a 'fyi'\n",
    "    raw_data = pd.DataFrame() # constructor\n",
    "    data_classes = {\n",
    "        'POS': {\n",
    "            'sentences': [] # array of strings\n",
    "            , 'words': [] # array of arrays, each array contains each sentence splitted\n",
    "            , 'words_without_stopwords': [] # same as words but without stopwords\n",
    "            , 'words_1d': [] # 1d array of words\n",
    "            , 'lemma': []\n",
    "            , 'bow': None\n",
    "            , 'ggram': None\n",
    "        }\n",
    "        , 'NEG': {}\n",
    "    }\n",
    "    stopwords = STOPWORDS # default if not given\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    def __init__(self, df, steps, nlp_model = 'en_core_web_lg', stopwords=STOPWORDS, ngrams=2, min_count=5, threshold=10, allowed_postags=ALLOWED_POSTAGS, debug = False):\n",
    "        # pandas dataframe?\n",
    "        self.stopwords = stopwords\n",
    "        self.raw_data = df\n",
    "        self.ngram = {'min_count': min_count, 'threshold': threshold, 'ngrams': ngrams}\n",
    "        self.steps = steps\n",
    "        self.allowed_postags = allowed_postags\n",
    "        self.debug = debug\n",
    "        if nlp_model not in NLP_MODELS:\n",
    "            nlp_model = 'en_core_web_lg'\n",
    "        self.nlp = NLP_MODELS[nlp_model]        \n",
    "    \n",
    "    def fit(self):\n",
    "        self.data_classes = {sentiment.value: {'sentences': self.raw_data[self.raw_data['sentiment'] == sentiment.value]['review'].values.tolist()} for sentiment in Sentiments}\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            print(f'{datetime.now()} {sentiment} start')\n",
    "            percentage = len(self.data_classes[sentiment]['sentences'])/len(self.raw_data.index)\n",
    "            sentences = self.data_classes[sentiment]['sentences']\n",
    "            print(f'{datetime.now()} sentences_as_words')\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            self.data_classes[sentiment]['percentage'] = percentage\n",
    "            if (self.debug):\n",
    "                self.data_classes[sentiment]['sentences_as_words'] = result\n",
    "            \n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['words_without_stopwords'] = result\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['lemmas'] = result\n",
    "                elif step == 'ngram':\n",
    "                    ngram_model = self.train_ngrams(result, ngrams=self.ngram['ngrams'], min_count=self.ngram['min_count'], threshold=self.ngram['threshold'])\n",
    "                    if len(ngram_model)>0:\n",
    "                        self.data_classes[sentiment]['ngram_model'] = ngram_model\n",
    "\n",
    "                        result = self.create_ngrams(ngram_model, result)\n",
    "                        if (self.debug):\n",
    "                            self.data_classes[sentiment]['ngrams'] = result\n",
    "                    else:\n",
    "                        print(f'{datetime.now()}ngram not done: {self.ngram[\"ngram\"]}')\n",
    "                else:\n",
    "                    print(f'{datetime.now()}instruction not found: {step}')\n",
    "            \n",
    "            words = self.array2dto1d(result)\n",
    "            self.data_classes[sentiment]['words'] = words\n",
    "            \n",
    "            # if not debug remove sentences \n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['sentences']\n",
    "            \n",
    "        print(f'{datetime.now()} probs')\n",
    "            \n",
    "        all_words = []\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            all_words.append(self.data_classes[sentiment]['words'])\n",
    "        self.dictionary = Dictionary(all_words)\n",
    "        \n",
    "        for sentiment in self.data_classes.keys():\n",
    "            self.data_classes[sentiment]['bow'] = self.dictionary.doc2bow(self.data_classes[sentiment]['words'])\n",
    "            self.data_classes[sentiment]['total_length'] = len(self.data_classes[sentiment]['words']) + len(self.dictionary)\n",
    "            word_probs = defaultdict(lambda: np.log(1/self.data_classes[sentiment]['total_length'])) # default value\n",
    "            for id, count in self.data_classes[sentiment]['bow']:\n",
    "                word_probs[self.dictionary[id]] = np.log((count + 1)/self.data_classes[sentiment]['total_length']) # {'word': prob}\n",
    "            self.data_classes[sentiment]['word_probs'] = word_probs\n",
    "\n",
    "            # if not debug, remove words\n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['words']\n",
    "                del self.data_classes[sentiment]['bow']\n",
    "        print(f'{datetime.now()} end')\n",
    "        \n",
    "    def predict_list(self, list_of_sentences):\n",
    "        results = []\n",
    "        for sentence in list_of_sentences:\n",
    "            results.append({'sentence': sentence, **self.predict(sentence)})\n",
    "        return results\n",
    "            \n",
    "    def predict(self, sentence):\n",
    "        sentences = [sentence]\n",
    "        probs = {}\n",
    "        selected = None\n",
    "        current_value_selected = float('-inf')\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                elif step == 'ngram':\n",
    "                    if 'ngram_model' in self.data_classes[sentiment] and len(self.data_classes[sentiment]['ngram_model'])>0:                    \n",
    "                        result = self.create_ngrams(self.data_classes[sentiment]['ngram_model'], result)\n",
    "                    else:\n",
    "                        print(f'no ngram model found for {sentiment}')\n",
    "                else:\n",
    "                    print(f'instruction not found: {step}')\n",
    "                print(f'{datetime.now()} {result}')\n",
    "            \n",
    "            prob_values = []\n",
    "            for one_row in result: # remember we added the sentence to an array\n",
    "                for word in one_row:\n",
    "                    prob_values.append(self.data_classes[sentiment]['word_probs'][word])\n",
    "            prob_values.append(np.log(self.data_classes[sentiment]['percentage']))\n",
    "            probs[sentiment] = {'prob': sum(prob_values), 'probs': prob_values}\n",
    "            if (probs[sentiment]['prob'] > current_value_selected):\n",
    "                current_value_selected = probs[sentiment]['prob'] \n",
    "                selected = sentiment\n",
    "        probs['selected'] = selected\n",
    "        return probs           \n",
    "    \n",
    "    def sentences2words(self, sentences):\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.append(simple_preprocess(sentence, deacc=True))\n",
    "        return words\n",
    "        \n",
    "    def remove_stopwords(self, list_of_list_of_words):\n",
    "        \"\"\"receives a list of list of words [['abc', 'abc', ...], ...] \"\"\"\n",
    "        words = []\n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            words.append([word for word in sentence_as_words if word not in self.stopwords])\n",
    "        return words\n",
    "            \n",
    "    def lemmatization(self, list_of_list_of_words):\n",
    "        words = []        \n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            doc = self.nlp(' '.join(sentence_as_words))\n",
    "            words.append([token.lemma_ for token in doc if token.pos_ in self.allowed_postags ])\n",
    "        return words\n",
    "    \n",
    "    def array2dto1d(self, array2d):\n",
    "        result = []\n",
    "        for array1d in array2d:\n",
    "            result.extend(array1d)\n",
    "        return result\n",
    "            \n",
    "    def train_ngrams(self, list_of_list_of_words, ngrams=2, min_count=5, threshold=10):\n",
    "        if ngrams < 2:\n",
    "            ngrams = 2\n",
    "        result = list_of_list_of_words\n",
    "        ngram_models = []\n",
    "        for i in range(ngrams-1):\n",
    "            ngram_phraser = Phrases(result, min_count=min_count, threshold=threshold)\n",
    "            ngram_model = Phraser(ngram_phraser)\n",
    "            ngram_models.append(ngram_model)\n",
    "            result = list(ngram_model[result])\n",
    "            \n",
    "        return ngram_models\n",
    "    \n",
    "    def create_ngrams(self, ngram_model_array, list_of_list_of_words):\n",
    "        \"\"\"ngram_model = []\"\"\"\n",
    "        result = list_of_list_of_words\n",
    "        for ngram_model in ngram_model_array:\n",
    "            result = list(ngram_model[result])\n",
    "        return result\n",
    "        \n",
    "        #dictionary.doc2idx(['abysmal', 'abuse'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895cfef",
   "metadata": {},
   "source": [
    "POS|DESCRIPTION|EXAMPLES\n",
    "---|---|---\n",
    "ADJ|adjective|*big, old, green, incomprehensible, first*\n",
    "ADP|adposition|*in, to, during*\n",
    "ADV|adverb|*very, tomorrow, down, where, there*\n",
    "AUX|auxiliary|*is, has (done), will (do), should (do)*\n",
    "CONJ|conjunction|*and, or, but*\n",
    "CCONJ|coordinating conjunction|*and, or, but*\n",
    "DET|determiner|*a, an, the*\n",
    "INTJ|interjection|*psst, ouch, bravo, hello*\n",
    "NOUN|noun|*girl, cat, tree, air, beauty*\n",
    "NUM|numeral|*1, 2017, one, seventy-seven, IV, MMXIV*\n",
    "PART|particle|*’s, not,*\n",
    "PRON|pronoun|*I, you, he, she, myself, themselves, somebody*\n",
    "PROPN|proper noun|*Mary, John, London, NATO, HBO*\n",
    "PUNCT|punctuation|*., (, ), ?*\n",
    "SCONJ|subordinating conjunction|*if, while, that*\n",
    "SYM|symbol|*$, %, §, ©, +, −, ×, ÷, =, :), *\n",
    "VERB|verb|*run, runs, running, eat, ate, eating*\n",
    "X|other|*sfpksdpsxmsa*\n",
    "SPACE|space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "971b8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    'remove_stopwords',\n",
    "    'lemmatization',\n",
    "    'ngram'\n",
    "]\n",
    "train = pd.read_csv('data/small.csv')\n",
    "# train = pd.read_csv('data/train.csv')\n",
    "# ALLOWED_POSTAGS=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "# ngrams starts from 2\n",
    "train_model = Model(train, steps\n",
    "                    , debug=False\n",
    "                    , nlp_model='en_core_web_lg'\n",
    "                    , ngrams=4\n",
    "                    , min_count=1\n",
    "                    , threshold=10\n",
    "                    , allowed_postags=ALLOWED_POSTAGS\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "3dcff6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 14:37:38.357396 POS start\n",
      "2022-07-01 14:37:38.357760 sentences_as_words\n",
      "2022-07-01 14:37:38.361191 remove_stopwords\n",
      "2022-07-01 14:37:38.361433 lemmatization\n",
      "2022-07-01 14:37:38.598272 ngram\n",
      "2022-07-01 14:37:38.610965 NEG start\n",
      "2022-07-01 14:37:38.611072 sentences_as_words\n",
      "2022-07-01 14:37:38.611774 remove_stopwords\n",
      "2022-07-01 14:37:38.611873 lemmatization\n",
      "2022-07-01 14:37:38.649939 ngram\n",
      "2022-07-01 14:37:38.653645 probs\n",
      "2022-07-01 14:37:38.656103 end\n"
     ]
    }
   ],
   "source": [
    "train_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "a23860df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POS': {'percentage': 0.8888888888888888,\n",
       "  'ngram_model': [<gensim.models.phrases.FrozenPhrases at 0x7f88183d6390>,\n",
       "   <gensim.models.phrases.FrozenPhrases at 0x7f88183d61d0>,\n",
       "   <gensim.models.phrases.FrozenPhrases at 0x7f88189adb38>],\n",
       "  'total_length': 537,\n",
       "  'word_probs': defaultdict(<function __main__.Model.fit.<locals>.<lambda>()>,\n",
       "              {'absolutely': -5.5928509139489195,\n",
       "               'access': -5.187385805840755,\n",
       "               'agency': -5.5928509139489195,\n",
       "               'agree': -5.5928509139489195,\n",
       "               'also': -5.187385805840755,\n",
       "               'arrive': -5.5928509139489195,\n",
       "               'atmosphere': -5.5928509139489195,\n",
       "               'attention': -5.5928509139489195,\n",
       "               'authentic': -5.5928509139489195,\n",
       "               'available': -5.5928509139489195,\n",
       "               'avoid': -5.5928509139489195,\n",
       "               'away': -5.5928509139489195,\n",
       "               'back': -5.187385805840755,\n",
       "               'basic': -5.5928509139489195,\n",
       "               'bathroom': -5.5928509139489195,\n",
       "               'beautiful': -4.899703733388974,\n",
       "               'bite': -5.5928509139489195,\n",
       "               'book': -5.5928509139489195,\n",
       "               'booker': -5.5928509139489195,\n",
       "               'bourbon': -5.5928509139489195,\n",
       "               'breakfast': -5.5928509139489195,\n",
       "               'budget': -5.5928509139489195,\n",
       "               'build': -5.5928509139489195,\n",
       "               'building': -5.5928509139489195,\n",
       "               'cafe': -5.5928509139489195,\n",
       "               'caring': -5.5928509139489195,\n",
       "               'central': -5.187385805840755,\n",
       "               'centraly': -5.5928509139489195,\n",
       "               'centre': -5.5928509139489195,\n",
       "               'cheap': -5.5928509139489195,\n",
       "               'church': -5.5928509139489195,\n",
       "               'clean': -4.676560182074764,\n",
       "               'close': -5.5928509139489195,\n",
       "               'club': -5.5928509139489195,\n",
       "               'coffee': -5.5928509139489195,\n",
       "               'comfortable': -5.187385805840755,\n",
       "               'comfy': -5.5928509139489195,\n",
       "               'computer': -5.5928509139489195,\n",
       "               'consistently': -5.5928509139489195,\n",
       "               'converted': -5.5928509139489195,\n",
       "               'courtyard': -5.5928509139489195,\n",
       "               'customer': -5.5928509139489195,\n",
       "               'day': -5.187385805840755,\n",
       "               'decent': -5.5928509139489195,\n",
       "               'deck': -5.5928509139489195,\n",
       "               'decorate': -5.5928509139489195,\n",
       "               'definitely': -5.187385805840755,\n",
       "               'desk': -5.5928509139489195,\n",
       "               'different': -5.5928509139489195,\n",
       "               'dish': -5.5928509139489195,\n",
       "               'distance': -5.5928509139489195,\n",
       "               'door': -5.5928509139489195,\n",
       "               'early': -5.187385805840755,\n",
       "               'earplug': -5.5928509139489195,\n",
       "               'eat': -5.5928509139489195,\n",
       "               'else': -5.5928509139489195,\n",
       "               'ensure': -5.5928509139489195,\n",
       "               'escape': -5.5928509139489195,\n",
       "               'even': -5.5928509139489195,\n",
       "               'everywhere': -5.5928509139489195,\n",
       "               'excellent': -5.187385805840755,\n",
       "               'family': -5.5928509139489195,\n",
       "               'fancy': -5.187385805840755,\n",
       "               'fantastic': -5.5928509139489195,\n",
       "               'far': -5.5928509139489195,\n",
       "               'feel': -5.5928509139489195,\n",
       "               'fernando': -5.5928509139489195,\n",
       "               'fi': -5.5928509139489195,\n",
       "               'flower': -5.5928509139489195,\n",
       "               'food': -5.5928509139489195,\n",
       "               'fountain': -5.187385805840755,\n",
       "               'free': -5.187385805840755,\n",
       "               'fresh': -5.5928509139489195,\n",
       "               'friendly': -5.187385805840755,\n",
       "               'front': -5.5928509139489195,\n",
       "               'fruit': -5.5928509139489195,\n",
       "               'garden': -5.5928509139489195,\n",
       "               'get': -4.899703733388974,\n",
       "               'give': -5.5928509139489195,\n",
       "               'go': -4.676560182074764,\n",
       "               'good': -4.899703733388974,\n",
       "               'great': -4.4942386252808095,\n",
       "               'great_rooftop': -5.187385805840755,\n",
       "               'guatemalan': -5.5928509139489195,\n",
       "               'hammock': -5.5928509139489195,\n",
       "               'heart': -5.5928509139489195,\n",
       "               'hector': -5.5928509139489195,\n",
       "               'help': -5.5928509139489195,\n",
       "               'helpful': -4.899703733388974,\n",
       "               'highly_recommend': -5.187385805840755,\n",
       "               'hike': -5.5928509139489195,\n",
       "               'historic': -5.5928509139489195,\n",
       "               'hope': -5.5928509139489195,\n",
       "               'hospedaje': -5.5928509139489195,\n",
       "               'host': -5.5928509139489195,\n",
       "               'hot': -5.5928509139489195,\n",
       "               'hotel': -4.899703733388974,\n",
       "               'internet': -4.899703733388974,\n",
       "               'kind': -5.5928509139489195,\n",
       "               'lady': -5.5928509139489195,\n",
       "               'late': -5.5928509139489195,\n",
       "               'level': -5.5928509139489195,\n",
       "               'lobby': -5.5928509139489195,\n",
       "               'locate': -5.187385805840755,\n",
       "               'location': -4.676560182074764,\n",
       "               'lonely': -5.5928509139489195,\n",
       "               'long': -5.187385805840755,\n",
       "               'look': -5.5928509139489195,\n",
       "               'love': -5.5928509139489195,\n",
       "               'lovely': -5.5928509139489195,\n",
       "               'main': -5.5928509139489195,\n",
       "               'make_sure': -5.187385805840755,\n",
       "               'manage': -5.5928509139489195,\n",
       "               'maybe': -5.5928509139489195,\n",
       "               'minute': -5.5928509139489195,\n",
       "               'minute_walk': -4.899703733388974,\n",
       "               'morning': -5.5928509139489195,\n",
       "               'mountain': -5.5928509139489195,\n",
       "               'narrow': -5.5928509139489195,\n",
       "               'need': -4.899703733388974,\n",
       "               'never': -5.187385805840755,\n",
       "               'next': -5.187385805840755,\n",
       "               'nice': -4.899703733388974,\n",
       "               'nice_clean': -5.187385805840755,\n",
       "               'nicely': -5.5928509139489195,\n",
       "               'night': -4.899703733388974,\n",
       "               'noise': -5.5928509139489195,\n",
       "               'offer': -5.187385805840755,\n",
       "               'old': -5.5928509139489195,\n",
       "               'opinion': -5.5928509139489195,\n",
       "               'option': -5.5928509139489195,\n",
       "               'other': -5.5928509139489195,\n",
       "               'park': -5.5928509139489195,\n",
       "               'pay': -5.5928509139489195,\n",
       "               'people': -5.5928509139489195,\n",
       "               'perfect': -5.5928509139489195,\n",
       "               'personalized': -5.5928509139489195,\n",
       "               'place': -4.899703733388974,\n",
       "               'place_stay': -5.187385805840755,\n",
       "               'planet': -5.5928509139489195,\n",
       "               'plaza': -5.5928509139489195,\n",
       "               'possible': -5.187385805840755,\n",
       "               'price': -5.5928509139489195,\n",
       "               'property': -5.5928509139489195,\n",
       "               'provide': -5.5928509139489195,\n",
       "               'quetzale': -5.5928509139489195,\n",
       "               'quickly': -5.5928509139489195,\n",
       "               'quiet': -5.187385805840755,\n",
       "               'really': -5.5928509139489195,\n",
       "               'recently': -5.5928509139489195,\n",
       "               'reception': -5.5928509139489195,\n",
       "               'recommend': -5.187385805840755,\n",
       "               'regret': -5.5928509139489195,\n",
       "               'relax': -5.5928509139489195,\n",
       "               'reliable': -5.5928509139489195,\n",
       "               'replacement': -5.5928509139489195,\n",
       "               'restaurant': -5.187385805840755,\n",
       "               'return': -5.187385805840755,\n",
       "               'review': -5.5928509139489195,\n",
       "               'rip': -5.5928509139489195,\n",
       "               'roof': -5.187385805840755,\n",
       "               'room': -4.4942386252808095,\n",
       "               'salvage': -5.5928509139489195,\n",
       "               'say': -5.5928509139489195,\n",
       "               'seat': -5.5928509139489195,\n",
       "               'see': -5.187385805840755,\n",
       "               'seem': -5.5928509139489195,\n",
       "               'service': -5.187385805840755,\n",
       "               'shop': -5.5928509139489195,\n",
       "               'short': -5.5928509139489195,\n",
       "               'show': -5.187385805840755,\n",
       "               'shower': -5.5928509139489195,\n",
       "               'small': -5.187385805840755,\n",
       "               'space': -5.5928509139489195,\n",
       "               'speak': -5.5928509139489195,\n",
       "               'spend': -5.5928509139489195,\n",
       "               'spotless': -5.5928509139489195,\n",
       "               'staff': -4.4942386252808095,\n",
       "               'start': -5.5928509139489195,\n",
       "               'stay': -4.206556552829029,\n",
       "               'street': -5.187385805840755,\n",
       "               'super': -5.5928509139489195,\n",
       "               'tea': -5.5928509139489195,\n",
       "               'thoughful': -5.5928509139489195,\n",
       "               'tienda': -5.5928509139489195,\n",
       "               'top': -5.5928509139489195,\n",
       "               'tour': -5.187385805840755,\n",
       "               'town': -5.5928509139489195,\n",
       "               'transportation': -5.187385805840755,\n",
       "               'trip': -5.5928509139489195,\n",
       "               'typical': -5.5928509139489195,\n",
       "               'use': -5.187385805840755,\n",
       "               'van': -5.5928509139489195,\n",
       "               'variety': -5.5928509139489195,\n",
       "               'view': -4.899703733388974,\n",
       "               'volcano': -5.5928509139489195,\n",
       "               'wait': -5.5928509139489195,\n",
       "               'walk': -4.899703733388974,\n",
       "               'water': -5.5928509139489195,\n",
       "               'way': -5.187385805840755,\n",
       "               'week': -5.5928509139489195,\n",
       "               'weekend': -5.5928509139489195,\n",
       "               'welcome': -5.5928509139489195,\n",
       "               'well': -5.187385805840755,\n",
       "               'wifi': -5.5928509139489195,\n",
       "               'wish': -5.5928509139489195,\n",
       "               'worn': -5.5928509139489195,\n",
       "               'year': -5.5928509139489195})},\n",
       " 'NEG': {'percentage': 0.1111111111111111,\n",
       "  'ngram_model': [<gensim.models.phrases.FrozenPhrases at 0x7f86fe39a550>,\n",
       "   <gensim.models.phrases.FrozenPhrases at 0x7f8832a287b8>,\n",
       "   <gensim.models.phrases.FrozenPhrases at 0x7f86fe39a978>],\n",
       "  'total_length': 292,\n",
       "  'word_probs': defaultdict(<function __main__.Model.fit.<locals>.<lambda>()>,\n",
       "              {'also': -4.983606621708336,\n",
       "               'back': -4.983606621708336,\n",
       "               'bathroom': -4.983606621708336,\n",
       "               'breakfast': -4.983606621708336,\n",
       "               'close': -4.983606621708336,\n",
       "               'eat': -4.983606621708336,\n",
       "               'flower': -4.983606621708336,\n",
       "               'food': -4.983606621708336,\n",
       "               'get': -4.983606621708336,\n",
       "               'great': -4.983606621708336,\n",
       "               'hotel': -4.983606621708336,\n",
       "               'look': -4.983606621708336,\n",
       "               'lovely': -4.983606621708336,\n",
       "               'people': -4.578141513600172,\n",
       "               'place': -4.983606621708336,\n",
       "               'room': -4.578141513600172,\n",
       "               'small': -4.983606621708336,\n",
       "               'stay': -4.578141513600172,\n",
       "               'view': -4.983606621708336,\n",
       "               'well': -4.578141513600172,\n",
       "               'amazing': -4.983606621708336,\n",
       "               'bad': -4.983606621708336,\n",
       "               'chair': -4.983606621708336,\n",
       "               'dark': -4.983606621708336,\n",
       "               'experience': -4.983606621708336,\n",
       "               'first': -4.983606621708336,\n",
       "               'floor': -4.983606621708336,\n",
       "               'foot': -4.983606621708336,\n",
       "               'fork': -4.983606621708336,\n",
       "               'glow': -4.983606621708336,\n",
       "               'incredibly': -4.983606621708336,\n",
       "               'like': -4.983606621708336,\n",
       "               'make': -4.983606621708336,\n",
       "               'many': -4.983606621708336,\n",
       "               'mere': -4.983606621708336,\n",
       "               'much': -4.983606621708336,\n",
       "               'part': -4.983606621708336,\n",
       "               'patio': -4.983606621708336,\n",
       "               'rattle': -4.983606621708336,\n",
       "               'read': -4.983606621708336,\n",
       "               'reservation': -4.983606621708336,\n",
       "               'smell': -4.983606621708336,\n",
       "               'sure': -4.983606621708336,\n",
       "               'table': -4.578141513600172,\n",
       "               'talk': -4.983606621708336,\n",
       "               'terrace': -4.983606621708336,\n",
       "               'toilet': -4.983606621708336,\n",
       "               'unsettling': -4.983606621708336,\n",
       "               'upstairs': -4.578141513600172})}}"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.data_classes # ['POS']['ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "f0305ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 14:37:58.330204 remove_stopwords\n",
      "2022-07-01 14:37:58.330310 [['hotel', 'trash']]\n",
      "2022-07-01 14:37:58.330357 lemmatization\n",
      "2022-07-01 14:37:58.344993 [['hotel', 'trash']]\n",
      "2022-07-01 14:37:58.345086 ngram\n",
      "2022-07-01 14:37:58.345229 [['hotel', 'trash']]\n",
      "2022-07-01 14:37:58.345379 remove_stopwords\n",
      "2022-07-01 14:37:58.345433 [['hotel', 'trash']]\n",
      "2022-07-01 14:37:58.345468 lemmatization\n",
      "2022-07-01 14:37:58.356144 [['hotel', 'trash']]\n",
      "2022-07-01 14:37:58.356570 ngram\n",
      "2022-07-01 14:37:58.356720 [['hotel', 'trash']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'POS': {'prob': -10.694240571313639,\n",
       "  'probs': [-4.899703733388974, -5.676753802268282, -0.11778303565638351]},\n",
       " 'NEG': {'prob': -12.857585001312838,\n",
       "  'probs': [-4.983606621708336, -5.676753802268282, -2.1972245773362196]},\n",
       " 'selected': 'POS'}"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.predict('the hotel was trash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "5ef08e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 14:38:02.885949 remove_stopwords\n",
      "2022-07-01 14:38:02.886050 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.886094 lemmatization\n",
      "2022-07-01 14:38:02.901635 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.901741 ngram\n",
      "2022-07-01 14:38:02.901909 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.902079 remove_stopwords\n",
      "2022-07-01 14:38:02.902136 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.902173 lemmatization\n",
      "2022-07-01 14:38:02.912638 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.912742 ngram\n",
      "2022-07-01 14:38:02.912869 [['hotel', 'dirty', 'noisy']]\n",
      "2022-07-01 14:38:02.913034 remove_stopwords\n",
      "2022-07-01 14:38:02.913088 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.913124 lemmatization\n",
      "2022-07-01 14:38:02.923571 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.923661 ngram\n",
      "2022-07-01 14:38:02.923777 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.923904 remove_stopwords\n",
      "2022-07-01 14:38:02.923958 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.923995 lemmatization\n",
      "2022-07-01 14:38:02.934030 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.934115 ngram\n",
      "2022-07-01 14:38:02.934220 [['hotel', 'clean']]\n",
      "2022-07-01 14:38:02.934351 remove_stopwords\n",
      "2022-07-01 14:38:02.935237 [['hello', 'world']]\n",
      "2022-07-01 14:38:02.935280 lemmatization\n",
      "2022-07-01 14:38:02.944852 [['world']]\n",
      "2022-07-01 14:38:02.944939 ngram\n",
      "2022-07-01 14:38:02.945038 [['world']]\n",
      "2022-07-01 14:38:02.945160 remove_stopwords\n",
      "2022-07-01 14:38:02.945210 [['hello', 'world']]\n",
      "2022-07-01 14:38:02.945245 lemmatization\n",
      "2022-07-01 14:38:02.955259 [['world']]\n",
      "2022-07-01 14:38:02.955344 ngram\n",
      "2022-07-01 14:38:02.955437 [['world']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'the hotel is dirty and noisy',\n",
       "  'POS': {'prob': -16.37099437358192,\n",
       "   'probs': [-4.899703733388974,\n",
       "    -5.676753802268282,\n",
       "    -5.676753802268282,\n",
       "    -0.11778303565638351]},\n",
       "  'NEG': {'prob': -18.534338803581118,\n",
       "   'probs': [-4.983606621708336,\n",
       "    -5.676753802268282,\n",
       "    -5.676753802268282,\n",
       "    -2.1972245773362196]},\n",
       "  'selected': 'POS'},\n",
       " {'sentence': 'the hotel is clean',\n",
       "  'POS': {'prob': -9.694046951120121,\n",
       "   'probs': [-4.899703733388974, -4.676560182074764, -0.11778303565638351]},\n",
       "  'NEG': {'prob': -12.857585001312838,\n",
       "   'probs': [-4.983606621708336, -5.676753802268282, -2.1972245773362196]},\n",
       "  'selected': 'POS'},\n",
       " {'sentence': 'hello world',\n",
       "  'POS': {'prob': -5.794536837924666,\n",
       "   'probs': [-5.676753802268282, -0.11778303565638351]},\n",
       "  'NEG': {'prob': -7.873978379604502,\n",
       "   'probs': [-5.676753802268282, -2.1972245773362196]},\n",
       "  'selected': 'POS'}]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.predict_list([\"the hotel is dirty and noisy\", \"the hotel is clean\", \"hello world\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ed24dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world']"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp('hello world')  if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV'] ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
