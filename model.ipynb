{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620af7ab",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7d1fb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/kegarcia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk, enum, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4c346",
   "metadata": {},
   "source": [
    "## Cargar modelos de spacy\n",
    "Se agregó como hiperparámetro el modelo de spacy, por lo que se cargaron 4 modelos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "327878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_POSTAGS = ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']\n",
    "NLP_MODELS = {\n",
    "    'en_core_web_trf': spacy.load('en_core_web_trf'),    \n",
    "    'en_core_web_sm': spacy.load('en_core_web_sm'),\n",
    "    'en_core_web_md': spacy.load('en_core_web_md'),\n",
    "    'en_core_web_lg': spacy.load('en_core_web_lg')\n",
    "}\n",
    "\n",
    "class Sentiments(enum.Enum):\n",
    "    POS = 'POS'\n",
    "    NEG = 'NEG'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d29d76",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bbc8b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # not necesarry but just as a 'fyi'\n",
    "    raw_data = pd.DataFrame() # constructor\n",
    "    data_classes = {\n",
    "        'POS': {\n",
    "            'sentences': [] # array of strings\n",
    "            , 'words': [] # array of arrays, each array contains each sentence splitted\n",
    "            , 'words_without_stopwords': [] # same as words but without stopwords\n",
    "            , 'words_1d': [] # 1d array of words\n",
    "            , 'lemma': []\n",
    "            , 'bow': None\n",
    "            , 'ggram': None\n",
    "        }\n",
    "        , 'NEG': {}\n",
    "    }\n",
    "    stopwords = STOPWORDS # default if not given\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    def __init__(self, df, steps, nlp_model = 'en_core_web_lg', stopwords=STOPWORDS, ngrams=2, min_count=5, threshold=10, allowed_postags=ALLOWED_POSTAGS, debug = False):\n",
    "        self.stopwords = stopwords\n",
    "        self.raw_data = df\n",
    "        self.ngram = {'min_count': min_count, 'threshold': threshold, 'ngrams': ngrams}\n",
    "        self.steps = steps\n",
    "        self.allowed_postags = allowed_postags\n",
    "        self.debug = debug\n",
    "        if nlp_model not in NLP_MODELS:\n",
    "            nlp_model = 'en_core_web_lg'\n",
    "        self.nlp = NLP_MODELS[nlp_model]\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        '''\n",
    "        train model using all parameters\n",
    "        '''\n",
    "        # get from raw data a df with data_classes = {'POS':{'sentences':[.. , ..]}, 'NEG':{'sentences':[.. , ..]}}\n",
    "        self.data_classes = {sentiment.value: {'sentences': self.raw_data[self.raw_data['sentiment'] == sentiment.value]['review'].values.tolist()} for sentiment in Sentiments}\n",
    "        \n",
    "        # iterate for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            print(f'{datetime.now()} {sentiment} class start')\n",
    "            \n",
    "            # get class percentage (ie: percentage = POS/total sentences)\n",
    "            percentage = len(self.data_classes[sentiment]['sentences'])/len(self.raw_data.index)\n",
    "            self.data_classes[sentiment]['percentage'] = percentage\n",
    "            \n",
    "            # split sentences into array of words\n",
    "            print(f'{datetime.now()}   sentences_as_words')\n",
    "            sentences = self.data_classes[sentiment]['sentences']\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            if (self.debug):\n",
    "                self.data_classes[sentiment]['sentences_as_words'] = result\n",
    "            \n",
    "            # iterate the pipeline\n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()}   {step}')\n",
    "                \n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['words_without_stopwords'] = result\n",
    "                \n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['lemmas'] = result\n",
    "                \n",
    "                elif step == 'ngram':\n",
    "                    # train ngram model\n",
    "                    ngram_model = self.train_ngrams(result, ngrams=self.ngram['ngrams'], min_count=self.ngram['min_count'], threshold=self.ngram['threshold'])\n",
    "                    \n",
    "                    if len(ngram_model)>0:\n",
    "                        self.data_classes[sentiment]['ngram_model'] = ngram_model\n",
    "                        \n",
    "                        # apply ngram model\n",
    "                        result = self.create_ngrams(ngram_model, result)\n",
    "                        if (self.debug):\n",
    "                            self.data_classes[sentiment]['ngrams'] = result\n",
    "                    else:\n",
    "                        print(f'{datetime.now()} ngram not done: {self.ngram[\"ngram\"]}')\n",
    "                \n",
    "                else:\n",
    "                    print(f'{datetime.now()} instruction not found: {step}')\n",
    "            \n",
    "            # save all words per sentiment class\n",
    "            words = self.array2dto1d(result)\n",
    "            self.data_classes[sentiment]['words'] = words\n",
    "            \n",
    "            # if not debug remove sentences \n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['sentences']\n",
    "            \n",
    "        print(f'{datetime.now()} probs')\n",
    "        \n",
    "        # build word dictionary of all sentiment classes\n",
    "        all_words = []\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            all_words.append(self.data_classes[sentiment]['words'])\n",
    "        self.dictionary = Dictionary(all_words)\n",
    "        \n",
    "        # calculate word probabilities for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            # calculate bag of words\n",
    "            self.data_classes[sentiment]['bow'] = self.dictionary.doc2bow(self.data_classes[sentiment]['words'])\n",
    "            \n",
    "            # calculate probability of each word\n",
    "            self.data_classes[sentiment]['total_length'] = len(self.data_classes[sentiment]['words']) + len(self.dictionary)\n",
    "            word_probs = defaultdict(lambda: np.log(1/self.data_classes[sentiment]['total_length'])) # default value\n",
    "            for id, count in self.data_classes[sentiment]['bow']:\n",
    "                word_probs[self.dictionary[id]] = np.log((count + 1)/self.data_classes[sentiment]['total_length']) # {'word': prob}\n",
    "            self.data_classes[sentiment]['word_probs'] = word_probs\n",
    "\n",
    "            # if not debug, remove words\n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['words']\n",
    "                del self.data_classes[sentiment]['bow']\n",
    "        print(f'{datetime.now()} end')\n",
    "        \n",
    "    \n",
    "    def predict(self, sentence, debug=False):\n",
    "        '''\n",
    "        predict sentiment class of one sentence\n",
    "        '''\n",
    "        sentences = [sentence]\n",
    "        probs = {}\n",
    "        selected = None\n",
    "        current_value_selected = float('-inf')\n",
    "        \n",
    "        # iterate for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            # apply pipeline to sentence\n",
    "            for step in self.steps:\n",
    "                if debug:\n",
    "                    print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                elif step == 'ngram':\n",
    "                    if 'ngram_model' in self.data_classes[sentiment] and len(self.data_classes[sentiment]['ngram_model'])>0:                    \n",
    "                        result = self.create_ngrams(self.data_classes[sentiment]['ngram_model'], result)\n",
    "                    else:\n",
    "                        print(f'no ngram model found for {sentiment}')\n",
    "                else:\n",
    "                    print(f'instruction not found: {step}')\n",
    "                if debug:\n",
    "                    print(f'{datetime.now()} {result}')\n",
    "            \n",
    "            # calculate sentiment probability\n",
    "            prob_values = []\n",
    "            for one_row in result: # remember we added the sentence to an array\n",
    "                for word in one_row:\n",
    "                    prob_values.append(self.data_classes[sentiment]['word_probs'][word])\n",
    "            prob_values.append(np.log(self.data_classes[sentiment]['percentage']))\n",
    "            probs[sentiment] = {'prob': sum(prob_values), 'probs': prob_values}\n",
    "            if (probs[sentiment]['prob'] > current_value_selected):\n",
    "                current_value_selected = probs[sentiment]['prob'] \n",
    "                selected = sentiment\n",
    "        probs['selected'] = selected\n",
    "        return probs           \n",
    "    \n",
    "    def sentences2words(self, sentences):\n",
    "        \"\"\"\n",
    "        receives a list of strings (sentences) ['hello world', 'test, sentence!'] \n",
    "        and returns for each sentence a split of its words: [['hello','world'], ['test','sentence']]\n",
    "        using gensim simple_preprocess function\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.append(simple_preprocess(sentence, deacc=True))\n",
    "            # alternative:\n",
    "            #words.append([i.strip() for i in re.split(',| |_|-|!|\\.|;|:', sentence) if len(i.strip())>0])\n",
    "        return words\n",
    "        \n",
    "    def remove_stopwords(self, list_of_list_of_words):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['abc', 'abc', ...], ...]\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            words.append([word for word in sentence_as_words if word not in self.stopwords])\n",
    "        return words\n",
    "            \n",
    "    def lemmatization(self, list_of_list_of_words):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['swimming','after','playing']]\n",
    "        and returns the same list with each words lemma: [['swim','after','play']]\n",
    "        \"\"\"\n",
    "        words = []        \n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            doc = self.nlp(' '.join(sentence_as_words))\n",
    "            words.append([token.lemma_ for token in doc if token.pos_ in self.allowed_postags ])\n",
    "        return words\n",
    "    \n",
    "    def array2dto1d(self, array2d):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['hello','world'],['test']] \n",
    "        and returns in a single list ['hello','world','test']\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for array1d in array2d:\n",
    "            result.extend(array1d)\n",
    "        return result\n",
    "            \n",
    "    def train_ngrams(self, list_of_list_of_words, ngrams=2, min_count=5, threshold=10):\n",
    "        if ngrams < 2:\n",
    "            ngrams = 2\n",
    "        result = list_of_list_of_words\n",
    "        ngram_models = []\n",
    "        for i in range(ngrams-1):\n",
    "            ngram_phraser = Phrases(result, min_count=min_count, threshold=threshold)\n",
    "            ngram_model = Phraser(ngram_phraser)\n",
    "            ngram_models.append(ngram_model)\n",
    "            result = list(ngram_model[result])\n",
    "            \n",
    "        return ngram_models\n",
    "    \n",
    "    def create_ngrams(self, ngram_model_array, list_of_list_of_words):\n",
    "        \"\"\"ngram_model = []\"\"\"\n",
    "        result = list_of_list_of_words\n",
    "        for ngram_model in ngram_model_array:\n",
    "            result = list(ngram_model[result])\n",
    "        return result\n",
    "        \n",
    "        #dictionary.doc2idx(['abysmal', 'abuse'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84856941",
   "metadata": {},
   "source": [
    "## Función para obtener las métricas de un modelo\n",
    "Obtiene todas las métricas al evaluar sobre un dataframe. El dataframe debe tener las columnas review y sentiment. Calcula:\n",
    "- **evaluated**: La cantidad de registros evaluados\n",
    "- **tp_rate**: Ratio de Verdaderos Positivos\n",
    "- **tn_rate**: Ratio de Verdaderos Negativos\n",
    "- **fp_rate**: Ratio de Falsos Positivos\n",
    "- **fn_rate**: Ratio de Falsos Negativos\n",
    "- **accuracy**: (tp + tn)/(tp+fp+fn+tn)\n",
    "- **precision**: tp / (tp + fp)\n",
    "- **recall**: tp / (tp + fn)\n",
    "- **f1**: (2 * precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "56ecd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, df):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    predicted_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        review = row['review']\n",
    "        original = row['sentiment']\n",
    "        \n",
    "        # predict\n",
    "        predicted = model.predict(review)['selected']\n",
    "        predicted_list.append(predicted)\n",
    "    \n",
    "        # calculate metrics\n",
    "        if predicted==Sentiments.POS.value and original==Sentiments.POS.value:\n",
    "            tp += 1\n",
    "        elif predicted==Sentiments.NEG.value and original==Sentiments.NEG.value:\n",
    "            tn += 1\n",
    "        elif predicted==Sentiments.POS.value and original==Sentiments.NEG.value:\n",
    "            fp += 1\n",
    "        elif predicted==Sentiments.NEG.value and original==Sentiments.POS.value:\n",
    "            fn += 1\n",
    "    \n",
    "    # calculate final metrics\n",
    "    accuracy = (tp + tn)/(tp+fp+fn+tn) if (tp+fp+fn+tn)>0 else 0\n",
    "    precision = tp / (tp + fp) if (tp+fp) > 0 else -1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else -1\n",
    "    f1 = 2*precision*recall/(precision + recall) if (precision + recall > 0) else -1\n",
    "    \n",
    "    return {\n",
    "        'evaluated':len(predicted_list),\n",
    "        'tp_rate':tp/len(predicted_list),\n",
    "        'tn_rate':tn/len(predicted_list),\n",
    "        'fp_rate':fp/len(predicted_list),\n",
    "        'fn_rate':fn/len(predicted_list),\n",
    "        'accuracy':accuracy,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'f1':f1,\n",
    "        #'results':predicted_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "458dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model_metrics):\n",
    "    print('    Evaluados: '+'%.0f'%model_metrics['evaluated'])\n",
    "    \n",
    "    # confusion matrix\n",
    "    print('      TP Rate: '+'%.4f'%model_metrics['tp_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"tp_rate\"]))\n",
    "    print('      FP Rate: '+'%.4f'%model_metrics['fp_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"fp_rate\"]))\n",
    "    print('      TN Rate: '+'%.4f'%model_metrics['tn_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"tn_rate\"]))\n",
    "    print('      FN Rate: '+'%.4f'%model_metrics['fn_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"fn_rate\"]))\n",
    "    \n",
    "    # calculated metrics\n",
    "    print('    Accuracy: '+'%.4f'%model_metrics['accuracy'])\n",
    "    print('    Precision: '+'%.4f'%model_metrics['precision'])\n",
    "    print('    Recall: '+'%.4f'%model_metrics['recall'])\n",
    "    print('    F1: '+'%.4f'%model_metrics['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b440cde",
   "metadata": {},
   "source": [
    "## Carga de datos para entrenar, validar y probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dd7bd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "validation = pd.read_csv('data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f062155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>This is one of the best hotels I've ever staye...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Everything about this hotel was awesome. The s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>Our tour group stayed here for two nights.  Th...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>Excellent service at Porta Hotel Antigua. From...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>I almost always stay at Hotel Antigua when I t...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>50</td>\n",
       "      <td>I was there with a Belize delegation of about ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10973</th>\n",
       "      <td>40</td>\n",
       "      <td>Last week I stayed at the Camino Real in Antig...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10974</th>\n",
       "      <td>50</td>\n",
       "      <td>My boyfriend was in Guate on business and we d...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>40</td>\n",
       "      <td>I stayed at Camino Real Antigua for a conferen...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>40</td>\n",
       "      <td>very nice little hotel, brand new, with lots o...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10977 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review sentiment\n",
       "0          50  This is one of the best hotels I've ever staye...       POS\n",
       "1          50  Everything about this hotel was awesome. The s...       POS\n",
       "2          50  Our tour group stayed here for two nights.  Th...       POS\n",
       "3          50  Excellent service at Porta Hotel Antigua. From...       POS\n",
       "4          50  I almost always stay at Hotel Antigua when I t...       POS\n",
       "...       ...                                                ...       ...\n",
       "10972      50  I was there with a Belize delegation of about ...       POS\n",
       "10973      40  Last week I stayed at the Camino Real in Antig...       POS\n",
       "10974      50  My boyfriend was in Guate on business and we d...       POS\n",
       "10975      40  I stayed at Camino Real Antigua for a conferen...       POS\n",
       "10976      40  very nice little hotel, brand new, with lots o...       POS\n",
       "\n",
       "[10977 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0c1c4c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>I would definitely stay here again in Antigua....</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>Great location, in the heart of historic centr...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>Not only the place is nice, clean and in an ex...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>We spent two nights and I wish we could have s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>I just recently returned from Antigua and my s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>50</td>\n",
       "      <td>We didn't have much of a plan upon arriving to...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>40</td>\n",
       "      <td>This hotel was very close to the parque centra...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>50</td>\n",
       "      <td>good experience i highly recommend,  the food...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>50</td>\n",
       "      <td>Centrally located....12 small rooms. Would vou...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>40</td>\n",
       "      <td>I came to the hostel early in the morning. The...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2353 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                             review sentiment\n",
       "0         50  I would definitely stay here again in Antigua....       POS\n",
       "1         40  Great location, in the heart of historic centr...       POS\n",
       "2         50  Not only the place is nice, clean and in an ex...       POS\n",
       "3         50  We spent two nights and I wish we could have s...       POS\n",
       "4         50  I just recently returned from Antigua and my s...       POS\n",
       "...      ...                                                ...       ...\n",
       "2348      50  We didn't have much of a plan upon arriving to...       POS\n",
       "2349      40  This hotel was very close to the parque centra...       POS\n",
       "2350      50   good experience i highly recommend,  the food...       POS\n",
       "2351      50  Centrally located....12 small rooms. Would vou...       POS\n",
       "2352      40  I came to the hostel early in the morning. The...       POS\n",
       "\n",
       "[2353 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12874a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Cucuruchos is a great place to spend your time...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>We booked this place through Booking for one n...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>An odd mix of positives and negatives : +ves h...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>We had been recommended this hostel because of...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>I knew this place is right underneath the Sky ...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>50</td>\n",
       "      <td>Very attractive rooms and grounds. The outside...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>50</td>\n",
       "      <td>SIMPLY - Very beautiful - extremely clean - qu...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>50</td>\n",
       "      <td>We stayed here for 3 nights and 1 night in the...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>40</td>\n",
       "      <td>This is a very nice boutique hotel.  Staff is ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>50</td>\n",
       "      <td>Lots to do - pool, beer pong, volcano boarding...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2352 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                             review sentiment\n",
       "0         50  Cucuruchos is a great place to spend your time...       POS\n",
       "1         30  We booked this place through Booking for one n...       NEG\n",
       "2         30  An odd mix of positives and negatives : +ves h...       NEG\n",
       "3         30  We had been recommended this hostel because of...       NEG\n",
       "4         20  I knew this place is right underneath the Sky ...       NEG\n",
       "...      ...                                                ...       ...\n",
       "2347      50  Very attractive rooms and grounds. The outside...       POS\n",
       "2348      50  SIMPLY - Very beautiful - extremely clean - qu...       POS\n",
       "2349      50  We stayed here for 3 nights and 1 night in the...       POS\n",
       "2350      40  This is a very nice boutique hotel.  Staff is ...       POS\n",
       "2351      50  Lots to do - pool, beer pong, volcano boarding...       POS\n",
       "\n",
       "[2352 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863f059",
   "metadata": {},
   "source": [
    "## Ejemplo modelo simple\n",
    "Se entrena un modelo simple para hacer pruebas del flujo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "971b8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-03 00:58:41.779274 POS class start\n",
      "2022-07-03 00:58:41.779398   sentences_as_words\n",
      "2022-07-03 00:58:45.619335   remove_stopwords\n",
      "2022-07-03 00:58:45.869781   ngram\n",
      "2022-07-03 00:58:49.411947 NEG class start\n",
      "2022-07-03 00:58:49.412734   sentences_as_words\n",
      "2022-07-03 00:58:50.064902   remove_stopwords\n",
      "2022-07-03 00:58:50.106881   ngram\n",
      "2022-07-03 00:58:50.752851 probs\n",
      "2022-07-03 00:58:51.532143 end\n"
     ]
    }
   ],
   "source": [
    "model_test = Model(\n",
    "    df = train\n",
    "    , steps = ['remove_stopwords','ngram']\n",
    "    , nlp_model = 'en_core_web_sm'\n",
    "    , ngrams = 2\n",
    "    , min_count = 5\n",
    "    , threshold = 10\n",
    "    , allowed_postags = ALLOWED_POSTAGS\n",
    "    , debug = False\n",
    ")\n",
    "model_test.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55b59c",
   "metadata": {},
   "source": [
    "**Sanity check** de un caso negativo y uno positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09dc5297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -21.14690692178164, 'probs': [-3.915511649867557, -9.478052593473269, -7.623772767263879, -0.1295699111769321]}, 'NEG': {'prob': -19.56974651206995, 'probs': [-4.28513024399396, -6.872302738240207, -6.304693302624039, -2.1076202272117475]}, 'selected': 'NEG'}\n",
      ">> Selected class NEG\n"
     ]
    }
   ],
   "source": [
    "result = model_test.predict(\"the hotel was dirty and noisy\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe0ebe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -15.605211573416558, 'probs': [-3.915511649867557, -4.785817268499889, -6.77431274387218, -0.1295699111769321]}, 'NEG': {'prob': -20.27825475622301, 'probs': [-4.28513024399396, -5.520297450598949, -8.365206834418355, -2.1076202272117475]}, 'selected': 'POS'}\n",
      ">> Selected class POS\n"
     ]
    }
   ],
   "source": [
    "result = model_test.predict(\"the hotel was very clean and I love it! :)\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c459",
   "metadata": {},
   "source": [
    "Probar con la data de validación para **obtener las métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3461fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8474 (1993)\n",
      "      FP Rate: 0.0753 (177)\n",
      "      TN Rate: 0.0595 (140)\n",
      "      FN Rate: 0.0179 (42)\n",
      "    Accuracy: 0.9069\n",
      "    Precision: 0.9184\n",
      "    Recall: 0.9794\n",
      "    F1: 0.9479\n"
     ]
    }
   ],
   "source": [
    "model_metrics = get_metrics(model=model_test, df=validation)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81159bed",
   "metadata": {},
   "source": [
    "## Definir todos los modelos a entrenar\n",
    "Se creó un dataframe para incluir todos los modelos con los que se desea entrenar para hacer pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a9278774",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "\n",
    "# ###################################\n",
    "# solo stopwords\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# solo ngrams\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 10\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 10, 'threshold': 50\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 3, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 4, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# stopwords y bigramas\n",
    "# variando min_count y threshold\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 1, 'threshold': 5\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 3, 'threshold': 5\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 10\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 30\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# modelos con stopwords y trigramas\n",
    "# variando min_count y threshold\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 4, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "\n",
    "# ###################################\n",
    "# solo lematizando\n",
    "# variando el modelo de spacy\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_sm', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_mg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_trf', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# lematizacion y con ngrams\n",
    "# variando el modelo de spacy\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_sm', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_md', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_trf', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 7, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# lematizacion y con ngrams\n",
    "# variando el listado de postags\n",
    "# anteriormente se habia agregado por\n",
    "# default 'PART', por lo que se prueba\n",
    "# quitarlo\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96af872",
   "metadata": {},
   "source": [
    "## Entrenar todos los modelos\n",
    "Se obtienen todas las métricas de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8f6cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "Start training model 1/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': None, 'allowed_postags': None, 'model': <__main__.Model object at 0x7f3c707b2588>}\n",
      "\n",
      "2022-07-03 00:46:25.590672 POS class start\n",
      "2022-07-03 00:46:25.590768   sentences_as_words\n",
      "2022-07-03 00:46:29.393514   remove_stopwords\n",
      "2022-07-03 00:46:29.608382 NEG class start\n",
      "2022-07-03 00:46:29.609053   sentences_as_words\n",
      "2022-07-03 00:46:30.256721   remove_stopwords\n",
      "2022-07-03 00:46:30.293597 probs\n",
      "2022-07-03 00:46:31.014803 end\n",
      "\n",
      "2022-07-03 00:46:31.032593 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8406 (1977)\n",
      "  tn_rate: 0.0753 (177)\n",
      "  fp_rate: 0.0595 (140)\n",
      "  fn_rate: 0.0247 (58)\n",
      "  accuracy: 0.9158\n",
      "  precision: 0.9339\n",
      "  recall: 0.9715\n",
      "  f1: 0.9523\n",
      "\n",
      "*******************************************\n",
      "Start training model 2/10\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 2, 'min_count': 10, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None, 'model': <__main__.Model object at 0x7f3c797ef470>}\n",
      "\n",
      "2022-07-03 00:46:33.872815 POS class start\n",
      "2022-07-03 00:46:33.872920   sentences_as_words\n",
      "2022-07-03 00:46:37.631589   ngram\n",
      "2022-07-03 00:46:43.171950 NEG class start\n",
      "2022-07-03 00:46:43.172189   sentences_as_words\n",
      "2022-07-03 00:46:43.835697   ngram\n",
      "2022-07-03 00:46:44.899508 probs\n",
      "2022-07-03 00:46:46.008881 end\n",
      "\n",
      "2022-07-03 00:46:46.035630 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8350 (1964)\n",
      "  tn_rate: 0.0719 (169)\n",
      "  fp_rate: 0.0629 (148)\n",
      "  fn_rate: 0.0302 (71)\n",
      "  accuracy: 0.9069\n",
      "  precision: 0.9299\n",
      "  recall: 0.9651\n",
      "  f1: 0.9472\n",
      "\n",
      "*******************************************\n",
      "Start training model 3/10\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 3, 'min_count': 10, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:46:49.809037 POS class start\n",
      "2022-07-03 00:46:49.809148   sentences_as_words\n",
      "2022-07-03 00:46:53.974303   ngram\n",
      "2022-07-03 00:47:05.195740 NEG class start\n",
      "2022-07-03 00:47:05.196159   sentences_as_words\n",
      "2022-07-03 00:47:05.865587   ngram\n",
      "2022-07-03 00:47:07.987663 probs\n",
      "2022-07-03 00:47:09.146643 end\n",
      "\n",
      "2022-07-03 00:47:09.183554 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8333 (1960)\n",
      "  tn_rate: 0.0714 (168)\n",
      "  fp_rate: 0.0634 (149)\n",
      "  fn_rate: 0.0319 (75)\n",
      "  accuracy: 0.9048\n",
      "  precision: 0.9294\n",
      "  recall: 0.9631\n",
      "  f1: 0.9459\n",
      "\n",
      "*******************************************\n",
      "Start training model 4/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 1, 'threshold': 5, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:47:13.644492 POS class start\n",
      "2022-07-03 00:47:13.644591   sentences_as_words\n",
      "2022-07-03 00:47:17.426984   remove_stopwords\n",
      "2022-07-03 00:47:17.658350   ngram\n",
      "2022-07-03 00:47:21.057531 NEG class start\n",
      "2022-07-03 00:47:21.057746   sentences_as_words\n",
      "2022-07-03 00:47:21.704223   remove_stopwords\n",
      "2022-07-03 00:47:21.744180   ngram\n",
      "2022-07-03 00:47:22.363705 probs\n",
      "2022-07-03 00:47:23.283696 end\n",
      "\n",
      "2022-07-03 00:47:23.304448 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8554 (2012)\n",
      "  tn_rate: 0.0446 (105)\n",
      "  fp_rate: 0.0901 (212)\n",
      "  fn_rate: 0.0098 (23)\n",
      "  accuracy: 0.9001\n",
      "  precision: 0.9047\n",
      "  recall: 0.9887\n",
      "  f1: 0.9448\n",
      "\n",
      "*******************************************\n",
      "Start training model 5/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 3, 'threshold': 5, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:47:26.676615 POS class start\n",
      "2022-07-03 00:47:26.676754   sentences_as_words\n",
      "2022-07-03 00:47:30.466983   remove_stopwords\n",
      "2022-07-03 00:47:30.728783   ngram\n",
      "2022-07-03 00:47:34.476485 NEG class start\n",
      "2022-07-03 00:47:34.476886   sentences_as_words\n",
      "2022-07-03 00:47:35.128005   remove_stopwords\n",
      "2022-07-03 00:47:35.169431   ngram\n",
      "2022-07-03 00:47:35.821633 probs\n",
      "2022-07-03 00:47:36.593537 end\n",
      "\n",
      "2022-07-03 00:47:36.615974 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8520 (2004)\n",
      "  tn_rate: 0.0548 (129)\n",
      "  fp_rate: 0.0799 (188)\n",
      "  fn_rate: 0.0132 (31)\n",
      "  accuracy: 0.9069\n",
      "  precision: 0.9142\n",
      "  recall: 0.9848\n",
      "  f1: 0.9482\n",
      "\n",
      "*******************************************\n",
      "Start training model 6/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 10, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:47:39.991173 POS class start\n",
      "2022-07-03 00:47:39.991273   sentences_as_words\n",
      "2022-07-03 00:47:43.766193   remove_stopwords\n",
      "2022-07-03 00:47:44.017498   ngram\n",
      "2022-07-03 00:47:47.592310 NEG class start\n",
      "2022-07-03 00:47:47.593162   sentences_as_words\n",
      "2022-07-03 00:47:48.241757   remove_stopwords\n",
      "2022-07-03 00:47:48.287262   ngram\n",
      "2022-07-03 00:47:48.928076 probs\n",
      "2022-07-03 00:47:49.699038 end\n",
      "\n",
      "2022-07-03 00:47:49.722473 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8474 (1993)\n",
      "  tn_rate: 0.0595 (140)\n",
      "  fp_rate: 0.0753 (177)\n",
      "  fn_rate: 0.0179 (42)\n",
      "  accuracy: 0.9069\n",
      "  precision: 0.9184\n",
      "  recall: 0.9794\n",
      "  f1: 0.9479\n",
      "\n",
      "*******************************************\n",
      "Start training model 7/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:47:53.081867 POS class start\n",
      "2022-07-03 00:47:53.081959   sentences_as_words\n",
      "2022-07-03 00:47:56.850994   remove_stopwords\n",
      "2022-07-03 00:47:57.120328   ngram\n",
      "2022-07-03 00:48:00.677328 NEG class start\n",
      "2022-07-03 00:48:00.678136   sentences_as_words\n",
      "2022-07-03 00:48:01.328045   remove_stopwords\n",
      "2022-07-03 00:48:01.371787   ngram\n",
      "2022-07-03 00:48:02.016188 probs\n",
      "2022-07-03 00:48:02.801822 end\n",
      "\n",
      "2022-07-03 00:48:02.825802 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8457 (1989)\n",
      "  tn_rate: 0.0646 (152)\n",
      "  fp_rate: 0.0702 (165)\n",
      "  fn_rate: 0.0196 (46)\n",
      "  accuracy: 0.9103\n",
      "  precision: 0.9234\n",
      "  recall: 0.9774\n",
      "  f1: 0.9496\n",
      "\n",
      "*******************************************\n",
      "Start training model 8/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 7, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:48:06.224819 POS class start\n",
      "2022-07-03 00:48:06.224928   sentences_as_words\n",
      "2022-07-03 00:48:10.002376   remove_stopwords\n",
      "2022-07-03 00:48:10.258429   ngram\n",
      "2022-07-03 00:48:14.169376 NEG class start\n",
      "2022-07-03 00:48:14.169593   sentences_as_words\n",
      "2022-07-03 00:48:14.835508   remove_stopwords\n",
      "2022-07-03 00:48:14.880166   ngram\n",
      "2022-07-03 00:48:15.527636 probs\n",
      "2022-07-03 00:48:16.290934 end\n",
      "\n",
      "2022-07-03 00:48:16.315265 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8431 (1983)\n",
      "  tn_rate: 0.0680 (160)\n",
      "  fp_rate: 0.0668 (157)\n",
      "  fn_rate: 0.0221 (52)\n",
      "  accuracy: 0.9111\n",
      "  precision: 0.9266\n",
      "  recall: 0.9744\n",
      "  f1: 0.9499\n",
      "\n",
      "*******************************************\n",
      "Start training model 9/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 3, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:48:19.716006 POS class start\n",
      "2022-07-03 00:48:19.716111   sentences_as_words\n",
      "2022-07-03 00:48:23.508686   remove_stopwords\n",
      "2022-07-03 00:48:23.758794   ngram\n",
      "2022-07-03 00:48:30.813831 NEG class start\n",
      "2022-07-03 00:48:30.814137   sentences_as_words\n",
      "2022-07-03 00:48:31.462677   remove_stopwords\n",
      "2022-07-03 00:48:31.503152   ngram\n",
      "2022-07-03 00:48:32.772759 probs\n",
      "2022-07-03 00:48:33.543814 end\n",
      "\n",
      "2022-07-03 00:48:33.567473 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8486 (1996)\n",
      "  tn_rate: 0.0587 (138)\n",
      "  fp_rate: 0.0761 (179)\n",
      "  fn_rate: 0.0166 (39)\n",
      "  accuracy: 0.9073\n",
      "  precision: 0.9177\n",
      "  recall: 0.9808\n",
      "  f1: 0.9482\n",
      "\n",
      "*******************************************\n",
      "Start training model 10/10\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 4, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-03 00:48:37.373434 POS class start\n",
      "2022-07-03 00:48:37.373534   sentences_as_words\n",
      "2022-07-03 00:48:41.163949   remove_stopwords\n",
      "2022-07-03 00:48:41.409500   ngram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-03 00:48:52.274110 NEG class start\n",
      "2022-07-03 00:48:52.274897   sentences_as_words\n",
      "2022-07-03 00:48:52.922402   remove_stopwords\n",
      "2022-07-03 00:48:52.982278   ngram\n",
      "2022-07-03 00:48:54.906532 probs\n",
      "2022-07-03 00:48:55.675279 end\n",
      "\n",
      "2022-07-03 00:48:55.699022 validation metrics\n",
      "  evaluados: 2352\n",
      "  tp_rate: 0.8495 (1998)\n",
      "  tn_rate: 0.0574 (135)\n",
      "  fp_rate: 0.0774 (182)\n",
      "  fn_rate: 0.0157 (37)\n",
      "  accuracy: 0.9069\n",
      "  precision: 0.9165\n",
      "  recall: 0.9818\n",
      "  f1: 0.9480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_results = []\n",
    "for model_to_train in models_to_train:\n",
    "    print('*******************************************')\n",
    "    print(f'Start training model {len(model_results)+1}/{len(models_to_train)}')\n",
    "    print('*******************************************')\n",
    "    print(model_to_train)\n",
    "    print('')\n",
    "    \n",
    "    # build model\n",
    "    model = Model(\n",
    "        df = train\n",
    "        , steps = model_to_train['steps']\n",
    "        , nlp_model = model_to_train['nlp_model']\n",
    "        , ngrams = model_to_train['ngrams']\n",
    "        , min_count = model_to_train['min_count']\n",
    "        , threshold = model_to_train['threshold']\n",
    "        , allowed_postags = model_to_train['allowed_postags']\n",
    "        , debug = False\n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    model.fit()\n",
    "    model_to_train['model'] = model\n",
    "    \n",
    "    # test model with validation dataframe\n",
    "    print('')\n",
    "    print(f'{datetime.now()} validation metrics')\n",
    "    model_metrics = get_metrics(model=model, df=validation)\n",
    "    print_metrics(model_metrics)\n",
    "    \n",
    "    # append results\n",
    "    model_results.append({**model_to_train, **model_metrics})\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e888f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.840561</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>0.933869</td>\n",
       "      <td>0.971499</td>\n",
       "      <td>0.952312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.929924</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.947191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.063350</td>\n",
       "      <td>0.031888</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.929350</td>\n",
       "      <td>0.963145</td>\n",
       "      <td>0.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.855442</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.900085</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.988698</td>\n",
       "      <td>0.944823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.054847</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.914234</td>\n",
       "      <td>0.984767</td>\n",
       "      <td>0.948190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.847364</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.979361</td>\n",
       "      <td>0.947919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.845663</td>\n",
       "      <td>0.064626</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.019558</td>\n",
       "      <td>0.910289</td>\n",
       "      <td>0.923398</td>\n",
       "      <td>0.977396</td>\n",
       "      <td>0.949630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.843112</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.066752</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.911139</td>\n",
       "      <td>0.926636</td>\n",
       "      <td>0.974447</td>\n",
       "      <td>0.949940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.848639</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.917701</td>\n",
       "      <td>0.980835</td>\n",
       "      <td>0.948219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.849490</td>\n",
       "      <td>0.057398</td>\n",
       "      <td>0.077381</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.916514</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.948043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       steps  ngrams  min_count  threshold nlp_model  \\\n",
       "0         [remove_stopwords]     NaN        NaN        NaN      None   \n",
       "1                    [ngram]     2.0       10.0       30.0      None   \n",
       "2                    [ngram]     3.0       10.0       30.0      None   \n",
       "3  [remove_stopwords, ngram]     2.0        1.0        5.0      None   \n",
       "4  [remove_stopwords, ngram]     2.0        3.0        5.0      None   \n",
       "5  [remove_stopwords, ngram]     2.0        5.0       10.0      None   \n",
       "6  [remove_stopwords, ngram]     2.0        5.0       20.0      None   \n",
       "7  [remove_stopwords, ngram]     2.0        7.0       30.0      None   \n",
       "8  [remove_stopwords, ngram]     3.0        5.0       20.0      None   \n",
       "9  [remove_stopwords, ngram]     4.0        5.0       20.0      None   \n",
       "\n",
       "  allowed_postags  evaluated   tp_rate   tn_rate   fp_rate   fn_rate  \\\n",
       "0            None       2352  0.840561  0.075255  0.059524  0.024660   \n",
       "1            None       2352  0.835034  0.071854  0.062925  0.030187   \n",
       "2            None       2352  0.833333  0.071429  0.063350  0.031888   \n",
       "3            None       2352  0.855442  0.044643  0.090136  0.009779   \n",
       "4            None       2352  0.852041  0.054847  0.079932  0.013180   \n",
       "5            None       2352  0.847364  0.059524  0.075255  0.017857   \n",
       "6            None       2352  0.845663  0.064626  0.070153  0.019558   \n",
       "7            None       2352  0.843112  0.068027  0.066752  0.022109   \n",
       "8            None       2352  0.848639  0.058673  0.076105  0.016582   \n",
       "9            None       2352  0.849490  0.057398  0.077381  0.015731   \n",
       "\n",
       "   accuracy  precision    recall        f1  \n",
       "0  0.915816   0.933869  0.971499  0.952312  \n",
       "1  0.906888   0.929924  0.965111  0.947191  \n",
       "2  0.904762   0.929350  0.963145  0.945946  \n",
       "3  0.900085   0.904676  0.988698  0.944823  \n",
       "4  0.906888   0.914234  0.984767  0.948190  \n",
       "5  0.906888   0.918433  0.979361  0.947919  \n",
       "6  0.910289   0.923398  0.977396  0.949630  \n",
       "7  0.911139   0.926636  0.974447  0.949940  \n",
       "8  0.907313   0.917701  0.980835  0.948219  \n",
       "9  0.906888   0.916514  0.981818  0.948043  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(model_results)\n",
    "df_results.drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f053b99",
   "metadata": {},
   "source": [
    "## Buscar mejor modelo\n",
    "Para el mejor modelo se seleccionó la métrica de F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5e297a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.840561</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.024660</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>0.933869</td>\n",
       "      <td>0.971499</td>\n",
       "      <td>0.952312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.843112</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.066752</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.911139</td>\n",
       "      <td>0.926636</td>\n",
       "      <td>0.974447</td>\n",
       "      <td>0.949940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.845663</td>\n",
       "      <td>0.064626</td>\n",
       "      <td>0.070153</td>\n",
       "      <td>0.019558</td>\n",
       "      <td>0.910289</td>\n",
       "      <td>0.923398</td>\n",
       "      <td>0.977396</td>\n",
       "      <td>0.949630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.848639</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.917701</td>\n",
       "      <td>0.980835</td>\n",
       "      <td>0.948219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>0.054847</td>\n",
       "      <td>0.079932</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.914234</td>\n",
       "      <td>0.984767</td>\n",
       "      <td>0.948190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.849490</td>\n",
       "      <td>0.057398</td>\n",
       "      <td>0.077381</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.916514</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.948043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.847364</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.918433</td>\n",
       "      <td>0.979361</td>\n",
       "      <td>0.947919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.929924</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.947191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.063350</td>\n",
       "      <td>0.031888</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.929350</td>\n",
       "      <td>0.963145</td>\n",
       "      <td>0.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.855442</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.900085</td>\n",
       "      <td>0.904676</td>\n",
       "      <td>0.988698</td>\n",
       "      <td>0.944823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       steps  ngrams  min_count  threshold nlp_model  \\\n",
       "0         [remove_stopwords]     NaN        NaN        NaN      None   \n",
       "7  [remove_stopwords, ngram]     2.0        7.0       30.0      None   \n",
       "6  [remove_stopwords, ngram]     2.0        5.0       20.0      None   \n",
       "8  [remove_stopwords, ngram]     3.0        5.0       20.0      None   \n",
       "4  [remove_stopwords, ngram]     2.0        3.0        5.0      None   \n",
       "9  [remove_stopwords, ngram]     4.0        5.0       20.0      None   \n",
       "5  [remove_stopwords, ngram]     2.0        5.0       10.0      None   \n",
       "1                    [ngram]     2.0       10.0       30.0      None   \n",
       "2                    [ngram]     3.0       10.0       30.0      None   \n",
       "3  [remove_stopwords, ngram]     2.0        1.0        5.0      None   \n",
       "\n",
       "  allowed_postags  evaluated   tp_rate   tn_rate   fp_rate   fn_rate  \\\n",
       "0            None       2352  0.840561  0.075255  0.059524  0.024660   \n",
       "7            None       2352  0.843112  0.068027  0.066752  0.022109   \n",
       "6            None       2352  0.845663  0.064626  0.070153  0.019558   \n",
       "8            None       2352  0.848639  0.058673  0.076105  0.016582   \n",
       "4            None       2352  0.852041  0.054847  0.079932  0.013180   \n",
       "9            None       2352  0.849490  0.057398  0.077381  0.015731   \n",
       "5            None       2352  0.847364  0.059524  0.075255  0.017857   \n",
       "1            None       2352  0.835034  0.071854  0.062925  0.030187   \n",
       "2            None       2352  0.833333  0.071429  0.063350  0.031888   \n",
       "3            None       2352  0.855442  0.044643  0.090136  0.009779   \n",
       "\n",
       "   accuracy  precision    recall        f1  \n",
       "0  0.915816   0.933869  0.971499  0.952312  \n",
       "7  0.911139   0.926636  0.974447  0.949940  \n",
       "6  0.910289   0.923398  0.977396  0.949630  \n",
       "8  0.907313   0.917701  0.980835  0.948219  \n",
       "4  0.906888   0.914234  0.984767  0.948190  \n",
       "9  0.906888   0.916514  0.981818  0.948043  \n",
       "5  0.906888   0.918433  0.979361  0.947919  \n",
       "1  0.906888   0.929924  0.965111  0.947191  \n",
       "2  0.904762   0.929350  0.963145  0.945946  \n",
       "3  0.900085   0.904676  0.988698  0.944823  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.sort_values(by=['f1'], ascending=False)\n",
    "df_results.drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0bc387fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.840561</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.02466</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>0.933869</td>\n",
       "      <td>0.971499</td>\n",
       "      <td>0.952312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                steps  ngrams  min_count  threshold nlp_model allowed_postags  \\\n",
       "0  [remove_stopwords]     NaN        NaN        NaN      None            None   \n",
       "\n",
       "   evaluated   tp_rate   tn_rate   fp_rate  fn_rate  accuracy  precision  \\\n",
       "0       2352  0.840561  0.075255  0.059524  0.02466  0.915816   0.933869   \n",
       "\n",
       "     recall        f1  \n",
       "0  0.971499  0.952312  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model with best F1\n",
    "df_results.head(1).drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9cb93",
   "metadata": {},
   "source": [
    "## Métricas en dataframe de Test\n",
    "Una vez seleccionado el mejor modelo se calculan las métricas utilizando el dataframe de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8278d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=df_results.iloc[0]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "17c7366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluados: 2353\n",
      "      TP Rate: 0.8440 (1986)\n",
      "      FP Rate: 0.0493 (116)\n",
      "      TN Rate: 0.0752 (177)\n",
      "      FN Rate: 0.0314 (74)\n",
      "    Accuracy: 0.9193\n",
      "    Precision: 0.9448\n",
      "    Recall: 0.9641\n",
      "    F1: 0.9543\n"
     ]
    }
   ],
   "source": [
    "# calculate test dataframe metrics with best model\n",
    "model_metrics = get_metrics(model=model, df=test)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d93fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6632f734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06723cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df995911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad2785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1659c374",
   "metadata": {},
   "source": [
    "## Postags disponibles en spacy\n",
    "Estos son los postags que incluye spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895cfef",
   "metadata": {},
   "source": [
    "POS|DESCRIPTION|EXAMPLES\n",
    "---|---|---\n",
    "ADJ|adjective|*big, old, green, incomprehensible, first*\n",
    "ADP|adposition|*in, to, during*\n",
    "ADV|adverb|*very, tomorrow, down, where, there*\n",
    "AUX|auxiliary|*is, has (done), will (do), should (do)*\n",
    "CONJ|conjunction|*and, or, but*\n",
    "CCONJ|coordinating conjunction|*and, or, but*\n",
    "DET|determiner|*a, an, the*\n",
    "INTJ|interjection|*psst, ouch, bravo, hello*\n",
    "NOUN|noun|*girl, cat, tree, air, beauty*\n",
    "NUM|numeral|*1, 2017, one, seventy-seven, IV, MMXIV*\n",
    "PART|particle|*’s, not,*\n",
    "PRON|pronoun|*I, you, he, she, myself, themselves, somebody*\n",
    "PROPN|proper noun|*Mary, John, London, NATO, HBO*\n",
    "PUNCT|punctuation|*., (, ), ?*\n",
    "SCONJ|subordinating conjunction|*if, while, that*\n",
    "SYM|symbol|*$, %, §, ©, +, −, ×, ÷, =, :), *\n",
    "VERB|verb|*run, runs, running, eat, ate, eating*\n",
    "X|other|*sfpksdpsxmsa*\n",
    "SPACE|space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
