{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "7d1fb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/kegarcia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, enum, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "327878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_POSTAGS = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "\n",
    "class Sentiments(enum.Enum):\n",
    "    POS = 'POS'\n",
    "    NEG = 'NEG'\n",
    "\n",
    "#### nlp = spacy.load('en_core_web_trf') # cosa rara\n",
    "    \n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# nlp = spacy.load('en_core_web_trf')\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "def custom_tokenizer(text):\n",
    "    return Doc(nlp.vocab, text.split(' '))\n",
    "# nlp.tokenizer = custom_tokenizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "bbc8b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # not necesarry but just as a 'fyi'\n",
    "    raw_data = pd.DataFrame() # constructor\n",
    "    data_classes = {\n",
    "        'POS': {\n",
    "            'sentences': [] # array of strings\n",
    "            , 'words': [] # array of arrays, each array contains each sentence splitted\n",
    "            , 'words_without_stopwords': [] # same as words but without stopwords\n",
    "            , 'words_1d': [] # 1d array of words\n",
    "            , 'lemma': []\n",
    "            , 'bow': None\n",
    "            , 'ggram': None\n",
    "        }\n",
    "        , 'NEG': {}\n",
    "    }\n",
    "    stopwords = STOPWORDS # default if not given\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    def __init__(self, df, steps, stopwords=STOPWORDS, min_count=5, threshold=10, allowed_postags=ALLOWED_POSTAGS):\n",
    "        # pandas dataframe?\n",
    "        self.stopwords = stopwords\n",
    "        self.raw_data = df\n",
    "        self.ngram = {'min_count': min_count, 'threshold': threshold}\n",
    "        self.steps = steps\n",
    "        self.allowed_postags = allowed_postags\n",
    "    \n",
    "    def fit(self):\n",
    "\n",
    "        self.data_classes = {sentiment.value: {'sentences': self.raw_data[self.raw_data['sentiment'] == sentiment.value]['review'].values.tolist()} for sentiment in Sentiments}\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            print(f'{datetime.now()} {sentiment} start')\n",
    "            percentage = len(self.data_classes[sentiment]['sentences'])/len(self.raw_data.index)\n",
    "            sentences = self.data_classes[sentiment]['sentences']\n",
    "            print(f'{datetime.now()} sentences_as_words')\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            self.data_classes[sentiment]['percentage'] = percentage\n",
    "            self.data_classes[sentiment]['sentences_as_words'] = result\n",
    "            \n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                    self.data_classes[sentiment]['words_without_stopwords'] = result\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                    self.data_classes[sentiment]['lemmas'] = result\n",
    "                elif step == 'ngram':\n",
    "                    ngram_model = self.train_ngrams(result, min_count=self.ngram['min_count'], threshold=self.ngram['threshold'])\n",
    "                    self.data_classes[sentiment]['ngram_model'] = ngram_model\n",
    "                    \n",
    "                    result = self.create_ngrams(ngram_model, result)\n",
    "                    self.data_classes[sentiment]['ngrams'] = result\n",
    "                else:\n",
    "                    print(f'instruction not found: {step}')\n",
    "            \n",
    "            words = self.array2dto1d(result)\n",
    "            self.data_classes[sentiment]['words'] = words\n",
    "            \n",
    "        print(f'{datetime.now()} probs')\n",
    "            \n",
    "        all_words = []\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            all_words.append(self.data_classes[sentiment]['words'])\n",
    "        self.dictionary = Dictionary(all_words)\n",
    "        \n",
    "        for sentiment in self.data_classes.keys():\n",
    "            self.data_classes[sentiment]['bow'] = self.dictionary.doc2bow(self.data_classes[sentiment]['words'])\n",
    "            self.data_classes[sentiment]['total_length'] = len(self.data_classes[sentiment]['words']) + len(self.dictionary)\n",
    "            word_probs = defaultdict(lambda: np.log(1/self.data_classes[sentiment]['total_length'])) # default value\n",
    "            for id, count in self.data_classes[sentiment]['bow']:\n",
    "                word_probs[self.dictionary[id]] = np.log((count + 1)/self.data_classes[sentiment]['total_length']) # {'word': prob}\n",
    "            self.data_classes[sentiment]['word_probs'] = word_probs\n",
    "        print(f'{datetime.now()} end')\n",
    "            \n",
    "    def predict(self, sentence):\n",
    "        sentences = [sentence]\n",
    "        probs = {}\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                elif step == 'ngram':\n",
    "                    if 'ngram_model' in self.data_classes[sentiment]:                    \n",
    "                        result = self.create_ngrams(self.data_classes[sentiment]['ngram_model'], result)\n",
    "                    else:\n",
    "                        print(f'no ngram model found for {sentiment}')\n",
    "                else:\n",
    "                    print(f'instruction not found: {step}')\n",
    "                print(f'{datetime.now()} {result}')\n",
    "            \n",
    "            prob_values = []\n",
    "            for one_row in result: # remember we added the sentence to an array\n",
    "                for word in one_row:\n",
    "                    prob_values.append(self.data_classes[sentiment]['word_probs'][word])\n",
    "            prob_values.append(np.log(self.data_classes[sentiment]['percentage']))\n",
    "            probs[sentiment] = {'prob': sum(prob_values), 'probs': prob_values}\n",
    "        return probs           \n",
    "    \n",
    "    def sentences2words(self, sentences):\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.append(simple_preprocess(sentence, deacc=True))\n",
    "        return words\n",
    "        \n",
    "    def remove_stopwords(self, list_of_list_of_words):\n",
    "        \"\"\"receives a list of list of words [['abc', 'abc', ...], ...] \"\"\"\n",
    "        words = []\n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            words.append([word for word in sentence_as_words if word not in self.stopwords])\n",
    "        return words\n",
    "            \n",
    "    def lemmatization(self, list_of_list_of_words):\n",
    "        words = []        \n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            doc = nlp(' '.join(sentence_as_words))\n",
    "            words.append([token.lemma_ for token in doc if token.pos_ in self.allowed_postags ])\n",
    "        return words\n",
    "    \n",
    "    def array2dto1d(self, array2d):\n",
    "        result = []\n",
    "        for array1d in array2d:\n",
    "            result.extend(array1d)\n",
    "        return result\n",
    "            \n",
    "    def train_ngrams(self, list_of_list_of_words, min_count=5, threshold=10):\n",
    "        bigram = Phrases(list_of_list_of_words, min_count=min_count, threshold=threshold)\n",
    "        bigram_mod = Phraser(bigram)\n",
    "        return bigram_mod\n",
    "    \n",
    "    def create_ngrams(self, ngram_model, list_of_list_of_words):\n",
    "        return list(ngram_model[list_of_list_of_words])\n",
    "        \n",
    "        #dictionary.doc2idx(['abysmal', 'abuse'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57bdb5",
   "metadata": {},
   "source": [
    "POS|DESCRIPTION|EXAMPLES\n",
    "---|---|---\n",
    "ADJ|adjective|*big, old, green, incomprehensible, first*\n",
    "ADP|adposition|*in, to, during*\n",
    "ADV|adverb|*very, tomorrow, down, where, there*\n",
    "AUX|auxiliary|*is, has (done), will (do), should (do)*\n",
    "CONJ|conjunction|*and, or, but*\n",
    "CCONJ|coordinating conjunction|*and, or, but*\n",
    "DET|determiner|*a, an, the*\n",
    "INTJ|interjection|*psst, ouch, bravo, hello*\n",
    "NOUN|noun|*girl, cat, tree, air, beauty*\n",
    "NUM|numeral|*1, 2017, one, seventy-seven, IV, MMXIV*\n",
    "PART|particle|*’s, not,*\n",
    "PRON|pronoun|*I, you, he, she, myself, themselves, somebody*\n",
    "PROPN|proper noun|*Mary, John, London, NATO, HBO*\n",
    "PUNCT|punctuation|*., (, ), ?*\n",
    "SCONJ|subordinating conjunction|*if, while, that*\n",
    "SYM|symbol|*$, %, §, ©, +, −, ×, ÷, =, :), *\n",
    "VERB|verb|*run, runs, running, eat, ate, eating*\n",
    "X|other|*sfpksdpsxmsa*\n",
    "SPACE|space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "971b8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    'remove_stopwords',\n",
    "    'lemmatization',\n",
    "    'ngram'\n",
    "]\n",
    "# train = pd.read_csv('data/small.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "# ALLOWED_POSTAGS=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "train_model = Model(train, steps, min_count=5, threshold=10, allowed_postags=ALLOWED_POSTAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "3dcff6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 07:11:52.378834 POS start\n",
      "2022-07-01 07:11:52.379073 sentences_as_words\n",
      "2022-07-01 07:11:56.265525 remove_stopwords\n",
      "2022-07-01 07:11:56.480715 lemmatization\n",
      "2022-07-01 07:15:01.589236 ngram\n",
      "2022-07-01 07:15:04.101109 NEG start\n",
      "2022-07-01 07:15:04.101409 sentences_as_words\n",
      "2022-07-01 07:15:04.786313 remove_stopwords\n",
      "2022-07-01 07:15:04.830509 lemmatization\n",
      "2022-07-01 07:15:31.209033 ngram\n",
      "2022-07-01 07:15:31.664149 probs\n",
      "2022-07-01 07:15:32.359753 end\n"
     ]
    }
   ],
   "source": [
    "train_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "f0305ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-01 07:16:23.488100 remove_stopwords\n",
      "2022-07-01 07:16:23.488364 [['hotel', 'trash']]\n",
      "2022-07-01 07:16:23.488411 lemmatization\n",
      "2022-07-01 07:16:23.504152 [['hotel', 'trash']]\n",
      "2022-07-01 07:16:23.504675 ngram\n",
      "2022-07-01 07:16:23.504801 [['hotel', 'trash']]\n",
      "2022-07-01 07:16:23.504968 remove_stopwords\n",
      "2022-07-01 07:16:23.505582 [['hotel', 'trash']]\n",
      "2022-07-01 07:16:23.505619 lemmatization\n",
      "2022-07-01 07:16:23.515265 [['hotel', 'trash']]\n",
      "2022-07-01 07:16:23.515356 ngram\n",
      "2022-07-01 07:16:23.515439 [['hotel', 'trash']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'POS': {'prob': -15.303188691058208,\n",
       "  'probs': [-3.786545695334639, -11.387073084546639, -0.1295699111769321]},\n",
       " 'NEG': {'prob': -15.52823752827241,\n",
       "  'probs': [-4.057827695896978, -9.362789605163684, -2.1076202272117475]}}"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.predict('the hotel was trash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "ed24dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['world']"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in nlp('hello world')  if token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV'] ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
