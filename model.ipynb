{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620af7ab",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1fb2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /data/home/kegarcia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk, enum, spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c4c346",
   "metadata": {},
   "source": [
    "## Cargar modelos de spacy\n",
    "Se agregó como hiperparámetro el modelo de spacy, por lo que se cargaron 4 modelos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "ALLOWED_POSTAGS = ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']\n",
    "NLP_MODELS = {\n",
    "    'en_core_web_trf': spacy.load('en_core_web_trf'),    \n",
    "    'en_core_web_sm': spacy.load('en_core_web_sm'),\n",
    "    'en_core_web_md': spacy.load('en_core_web_md'),\n",
    "    'en_core_web_lg': spacy.load('en_core_web_lg')\n",
    "}\n",
    "\n",
    "class Sentiments(enum.Enum):\n",
    "    POS = 'POS'\n",
    "    NEG = 'NEG'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d29d76",
   "metadata": {},
   "source": [
    "## Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc8b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # not necesarry but just as a 'fyi'\n",
    "    raw_data = pd.DataFrame() # constructor\n",
    "    data_classes = {\n",
    "        'POS': {\n",
    "            'sentences': [] # array of strings\n",
    "            , 'words': [] # array of arrays, each array contains each sentence splitted\n",
    "            , 'words_without_stopwords': [] # same as words but without stopwords\n",
    "            , 'words_1d': [] # 1d array of words\n",
    "            , 'lemma': []\n",
    "            , 'bow': None\n",
    "            , 'ggram': None\n",
    "        }\n",
    "        , 'NEG': {}\n",
    "    }\n",
    "    stopwords = STOPWORDS # default if not given\n",
    "    allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "    \n",
    "    def __init__(self, df, steps, nlp_model = 'en_core_web_lg', stopwords=STOPWORDS, ngrams=2, min_count=5, threshold=10, allowed_postags=ALLOWED_POSTAGS, debug = False):\n",
    "        self.stopwords = stopwords\n",
    "        self.raw_data = df\n",
    "        self.ngram = {'min_count': min_count, 'threshold': threshold, 'ngrams': ngrams}\n",
    "        self.steps = steps\n",
    "        self.allowed_postags = allowed_postags\n",
    "        self.debug = debug\n",
    "        if nlp_model not in NLP_MODELS:\n",
    "            nlp_model = 'en_core_web_lg'\n",
    "        self.nlp = NLP_MODELS[nlp_model]\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        '''\n",
    "        train model using all parameters\n",
    "        '''\n",
    "        # get from raw data a df with data_classes = {'POS':{'sentences':[.. , ..]}, 'NEG':{'sentences':[.. , ..]}}\n",
    "        self.data_classes = {sentiment.value: {'sentences': self.raw_data[self.raw_data['sentiment'] == sentiment.value]['review'].values.tolist()} for sentiment in Sentiments}\n",
    "        \n",
    "        # iterate for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            print(f'{datetime.now()} {sentiment} class start')\n",
    "            \n",
    "            # get class percentage (ie: percentage = POS/total sentences)\n",
    "            percentage = len(self.data_classes[sentiment]['sentences'])/len(self.raw_data.index)\n",
    "            self.data_classes[sentiment]['percentage'] = percentage\n",
    "            \n",
    "            # split sentences into array of words\n",
    "            print(f'{datetime.now()}   sentences_as_words')\n",
    "            sentences = self.data_classes[sentiment]['sentences']\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            if (self.debug):\n",
    "                self.data_classes[sentiment]['sentences_as_words'] = result\n",
    "            \n",
    "            # iterate the pipeline\n",
    "            for step in self.steps:\n",
    "                print(f'{datetime.now()}   {step}')\n",
    "                \n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['words_without_stopwords'] = result\n",
    "                \n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                    if (self.debug):\n",
    "                        self.data_classes[sentiment]['lemmas'] = result\n",
    "                \n",
    "                elif step == 'ngram':\n",
    "                    # train ngram model\n",
    "                    ngram_model = self.train_ngrams(result, ngrams=self.ngram['ngrams'], min_count=self.ngram['min_count'], threshold=self.ngram['threshold'])\n",
    "                    \n",
    "                    if len(ngram_model)>0:\n",
    "                        self.data_classes[sentiment]['ngram_model'] = ngram_model\n",
    "                        \n",
    "                        # apply ngram model\n",
    "                        result = self.create_ngrams(ngram_model, result)\n",
    "                        if (self.debug):\n",
    "                            self.data_classes[sentiment]['ngrams'] = result\n",
    "                    else:\n",
    "                        print(f'{datetime.now()} ngram not done: {self.ngram[\"ngram\"]}')\n",
    "                \n",
    "                else:\n",
    "                    print(f'{datetime.now()} instruction not found: {step}')\n",
    "            \n",
    "            # save all words per sentiment class\n",
    "            words = self.array2dto1d(result)\n",
    "            self.data_classes[sentiment]['words'] = words\n",
    "            \n",
    "            # if not debug remove sentences \n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['sentences']\n",
    "            \n",
    "        print(f'{datetime.now()} probs')\n",
    "        \n",
    "        # build word dictionary of all sentiment classes\n",
    "        all_words = []\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            all_words.append(self.data_classes[sentiment]['words'])\n",
    "        self.dictionary = Dictionary(all_words)\n",
    "        \n",
    "        # calculate word probabilities for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            # calculate bag of words\n",
    "            self.data_classes[sentiment]['bow'] = self.dictionary.doc2bow(self.data_classes[sentiment]['words'])\n",
    "            \n",
    "            # calculate probability of each word\n",
    "            self.data_classes[sentiment]['total_length'] = len(self.data_classes[sentiment]['words']) + len(self.dictionary)\n",
    "            self.data_classes[sentiment]['default_value_prop']= np.log(1/self.data_classes[sentiment]['total_length'])\n",
    "            word_probs = {}\n",
    "            for id, count in self.data_classes[sentiment]['bow']:\n",
    "                word_probs[self.dictionary[id]] = np.log((count + 1)/self.data_classes[sentiment]['total_length']) # {'word': prob}\n",
    "            self.data_classes[sentiment]['word_probs'] = word_probs\n",
    "\n",
    "            # if not debug, remove words\n",
    "            if (not self.debug):\n",
    "                del self.data_classes[sentiment]['words']\n",
    "                del self.data_classes[sentiment]['bow']\n",
    "        print(f'{datetime.now()} end')\n",
    "        \n",
    "    \n",
    "    def predict(self, sentence, debug=False):\n",
    "        '''\n",
    "        predict sentiment class of one sentence\n",
    "        '''\n",
    "        sentences = [sentence]\n",
    "        probs = {}\n",
    "        selected = None\n",
    "        current_value_selected = float('-inf')\n",
    "        \n",
    "        # iterate for each sentiment class\n",
    "        for sentiment in self.data_classes.keys():\n",
    "            result = self.sentences2words(sentences)\n",
    "            \n",
    "            # apply pipeline to sentence\n",
    "            for step in self.steps:\n",
    "                if debug:\n",
    "                    print(f'{datetime.now()} {step}')\n",
    "                if step == 'remove_stopwords':\n",
    "                    result = self.remove_stopwords(result)\n",
    "                elif step == 'lemmatization':\n",
    "                    result = self.lemmatization(result)\n",
    "                elif step == 'ngram':\n",
    "                    if 'ngram_model' in self.data_classes[sentiment] and len(self.data_classes[sentiment]['ngram_model'])>0:                    \n",
    "                        result = self.create_ngrams(self.data_classes[sentiment]['ngram_model'], result)\n",
    "                    else:\n",
    "                        print(f'no ngram model found for {sentiment}')\n",
    "                else:\n",
    "                    print(f'instruction not found: {step}')\n",
    "                if debug:\n",
    "                    print(f'{datetime.now()} {result}')\n",
    "            \n",
    "            # calculate sentiment probability\n",
    "            prob_values = []\n",
    "            for one_row in result: # remember we added the sentence to an array\n",
    "                for word in one_row:\n",
    "                    prob_values.append(self.data_classes[sentiment]['word_probs'].get(word, self.data_classes[sentiment]['default_value_prop']))\n",
    "            prob_values.append(np.log(self.data_classes[sentiment]['percentage']))\n",
    "            probs[sentiment] = {'prob': sum(prob_values), 'probs': prob_values}\n",
    "            if (probs[sentiment]['prob'] > current_value_selected):\n",
    "                current_value_selected = probs[sentiment]['prob'] \n",
    "                selected = sentiment\n",
    "        probs['selected'] = selected\n",
    "        return probs           \n",
    "    \n",
    "    def sentences2words(self, sentences):\n",
    "        \"\"\"\n",
    "        receives a list of strings (sentences) ['hello world', 'test, sentence!'] \n",
    "        and returns for each sentence a split of its words: [['hello','world'], ['test','sentence']]\n",
    "        using gensim simple_preprocess function\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.append(simple_preprocess(sentence, deacc=True))\n",
    "            # alternative:\n",
    "            #words.append([i.strip() for i in re.split(',| |_|-|!|\\.|;|:', sentence) if len(i.strip())>0])\n",
    "        return words\n",
    "        \n",
    "    def remove_stopwords(self, list_of_list_of_words):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['abc', 'abc', ...], ...]\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            words.append([word for word in sentence_as_words if word not in self.stopwords])\n",
    "        return words\n",
    "            \n",
    "    def lemmatization(self, list_of_list_of_words):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['swimming','after','playing']]\n",
    "        and returns the same list with each words lemma: [['swim','after','play']]\n",
    "        \"\"\"\n",
    "        words = []        \n",
    "        for sentence_as_words in list_of_list_of_words:\n",
    "            doc = self.nlp(' '.join(sentence_as_words))\n",
    "            words.append([token.lemma_ for token in doc if token.pos_ in self.allowed_postags ])\n",
    "        return words\n",
    "    \n",
    "    def array2dto1d(self, array2d):\n",
    "        \"\"\"\n",
    "        receives a list of list of words [['hello','world'],['test']] \n",
    "        and returns in a single list ['hello','world','test']\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for array1d in array2d:\n",
    "            result.extend(array1d)\n",
    "        return result\n",
    "            \n",
    "    def train_ngrams(self, list_of_list_of_words, ngrams=2, min_count=5, threshold=10):\n",
    "        if ngrams < 2:\n",
    "            ngrams = 2\n",
    "        result = list_of_list_of_words\n",
    "        ngram_models = []\n",
    "        for i in range(ngrams-1):\n",
    "            ngram_phraser = Phrases(result, min_count=min_count, threshold=threshold)\n",
    "            ngram_model = Phraser(ngram_phraser)\n",
    "            ngram_models.append(ngram_model)\n",
    "            result = list(ngram_model[result])\n",
    "            \n",
    "        return ngram_models\n",
    "    \n",
    "    def create_ngrams(self, ngram_model_array, list_of_list_of_words):\n",
    "        \"\"\"ngram_model = []\"\"\"\n",
    "        result = list_of_list_of_words\n",
    "        for ngram_model in ngram_model_array:\n",
    "            result = list(ngram_model[result])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84856941",
   "metadata": {},
   "source": [
    "## Función para obtener las métricas de un modelo\n",
    "Obtiene todas las métricas al evaluar sobre un dataframe. El dataframe debe tener las columnas review y sentiment. Calcula:\n",
    "- **evaluated**: La cantidad de registros evaluados\n",
    "- **tp_rate**: Ratio de Verdaderos Positivos\n",
    "- **tn_rate**: Ratio de Verdaderos Negativos\n",
    "- **fp_rate**: Ratio de Falsos Positivos\n",
    "- **fn_rate**: Ratio de Falsos Negativos\n",
    "- **accuracy**: (tp + tn)/(tp+fp+fn+tn)\n",
    "- **precision**: tp / (tp + fp)\n",
    "- **recall**: tp / (tp + fn)\n",
    "- **f1**: (2 * precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ecd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, df):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    predicted_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        review = row['review']\n",
    "        original = row['sentiment']\n",
    "        \n",
    "        # predict\n",
    "        predicted = model.predict(review)['selected']\n",
    "        predicted_list.append(predicted)\n",
    "    \n",
    "        # calculate metrics\n",
    "        if predicted==Sentiments.POS.value and original==Sentiments.POS.value:\n",
    "            tp += 1\n",
    "        elif predicted==Sentiments.NEG.value and original==Sentiments.NEG.value:\n",
    "            tn += 1\n",
    "        elif predicted==Sentiments.POS.value and original==Sentiments.NEG.value:\n",
    "            fp += 1\n",
    "        elif predicted==Sentiments.NEG.value and original==Sentiments.POS.value:\n",
    "            fn += 1\n",
    "    \n",
    "    # calculate final metrics\n",
    "    accuracy = (tp + tn)/(tp+fp+fn+tn) if (tp+fp+fn+tn)>0 else 0\n",
    "    precision = tp / (tp + fp) if (tp+fp) > 0 else -1\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else -1\n",
    "    f1 = 2*precision*recall/(precision + recall) if (precision + recall > 0) else -1\n",
    "    \n",
    "    return {\n",
    "        'evaluated':len(predicted_list),\n",
    "        'f1':f1,\n",
    "        'precision':precision,\n",
    "        'recall':recall,\n",
    "        'accuracy':accuracy,\n",
    "        'tp_rate':tp/len(predicted_list),\n",
    "        'tn_rate':tn/len(predicted_list),\n",
    "        'fp_rate':fp/len(predicted_list),\n",
    "        'fn_rate':fn/len(predicted_list),\n",
    "        \n",
    "        #'results':predicted_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458dbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model_metrics):\n",
    "    print('    Evaluados: '+'%.0f'%model_metrics['evaluated'])\n",
    "    \n",
    "    # confusion matrix\n",
    "    print('      TP Rate: '+'%.4f'%model_metrics['tp_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"tp_rate\"]))\n",
    "    print('      FP Rate: '+'%.4f'%model_metrics['fp_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"fp_rate\"]))\n",
    "    print('      TN Rate: '+'%.4f'%model_metrics['tn_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"tn_rate\"]))\n",
    "    print('      FN Rate: '+'%.4f'%model_metrics['fn_rate'] + ' (%.0f)'%(model_metrics[\"evaluated\"]*model_metrics[\"fn_rate\"]))\n",
    "    \n",
    "    # calculated metrics\n",
    "    print('    Accuracy: '+'%.4f'%model_metrics['accuracy'])\n",
    "    print('    Precision: '+'%.4f'%model_metrics['precision'])\n",
    "    print('    Recall: '+'%.4f'%model_metrics['recall'])\n",
    "    print('    F1: '+'%.4f'%model_metrics['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b440cde",
   "metadata": {},
   "source": [
    "## Carga de datos para entrenar, validar y probar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7bd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "validation = pd.read_csv('data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f062155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>This is one of the best hotels I've ever staye...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Everything about this hotel was awesome. The s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>Our tour group stayed here for two nights.  Th...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>Excellent service at Porta Hotel Antigua. From...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>I almost always stay at Hotel Antigua when I t...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>50</td>\n",
       "      <td>I was there with a Belize delegation of about ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10973</th>\n",
       "      <td>40</td>\n",
       "      <td>Last week I stayed at the Camino Real in Antig...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10974</th>\n",
       "      <td>50</td>\n",
       "      <td>My boyfriend was in Guate on business and we d...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>40</td>\n",
       "      <td>I stayed at Camino Real Antigua for a conferen...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>40</td>\n",
       "      <td>very nice little hotel, brand new, with lots o...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10977 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                             review sentiment\n",
       "0          50  This is one of the best hotels I've ever staye...       POS\n",
       "1          50  Everything about this hotel was awesome. The s...       POS\n",
       "2          50  Our tour group stayed here for two nights.  Th...       POS\n",
       "3          50  Excellent service at Porta Hotel Antigua. From...       POS\n",
       "4          50  I almost always stay at Hotel Antigua when I t...       POS\n",
       "...       ...                                                ...       ...\n",
       "10972      50  I was there with a Belize delegation of about ...       POS\n",
       "10973      40  Last week I stayed at the Camino Real in Antig...       POS\n",
       "10974      50  My boyfriend was in Guate on business and we d...       POS\n",
       "10975      40  I stayed at Camino Real Antigua for a conferen...       POS\n",
       "10976      40  very nice little hotel, brand new, with lots o...       POS\n",
       "\n",
       "[10977 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1c4c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>I would definitely stay here again in Antigua....</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>Great location, in the heart of historic centr...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>Not only the place is nice, clean and in an ex...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>We spent two nights and I wish we could have s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>I just recently returned from Antigua and my s...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>50</td>\n",
       "      <td>We didn't have much of a plan upon arriving to...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>40</td>\n",
       "      <td>This hotel was very close to the parque centra...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>50</td>\n",
       "      <td>good experience i highly recommend,  the food...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>50</td>\n",
       "      <td>Centrally located....12 small rooms. Would vou...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>40</td>\n",
       "      <td>I came to the hostel early in the morning. The...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2353 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                             review sentiment\n",
       "0         50  I would definitely stay here again in Antigua....       POS\n",
       "1         40  Great location, in the heart of historic centr...       POS\n",
       "2         50  Not only the place is nice, clean and in an ex...       POS\n",
       "3         50  We spent two nights and I wish we could have s...       POS\n",
       "4         50  I just recently returned from Antigua and my s...       POS\n",
       "...      ...                                                ...       ...\n",
       "2348      50  We didn't have much of a plan upon arriving to...       POS\n",
       "2349      40  This hotel was very close to the parque centra...       POS\n",
       "2350      50   good experience i highly recommend,  the food...       POS\n",
       "2351      50  Centrally located....12 small rooms. Would vou...       POS\n",
       "2352      40  I came to the hostel early in the morning. The...       POS\n",
       "\n",
       "[2353 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12874a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Cucuruchos is a great place to spend your time...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>We booked this place through Booking for one n...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>An odd mix of positives and negatives : +ves h...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>We had been recommended this hostel because of...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>I knew this place is right underneath the Sky ...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>50</td>\n",
       "      <td>Very attractive rooms and grounds. The outside...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>50</td>\n",
       "      <td>SIMPLY - Very beautiful - extremely clean - qu...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>50</td>\n",
       "      <td>We stayed here for 3 nights and 1 night in the...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2350</th>\n",
       "      <td>40</td>\n",
       "      <td>This is a very nice boutique hotel.  Staff is ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>50</td>\n",
       "      <td>Lots to do - pool, beer pong, volcano boarding...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2352 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rating                                             review sentiment\n",
       "0         50  Cucuruchos is a great place to spend your time...       POS\n",
       "1         30  We booked this place through Booking for one n...       NEG\n",
       "2         30  An odd mix of positives and negatives : +ves h...       NEG\n",
       "3         30  We had been recommended this hostel because of...       NEG\n",
       "4         20  I knew this place is right underneath the Sky ...       NEG\n",
       "...      ...                                                ...       ...\n",
       "2347      50  Very attractive rooms and grounds. The outside...       POS\n",
       "2348      50  SIMPLY - Very beautiful - extremely clean - qu...       POS\n",
       "2349      50  We stayed here for 3 nights and 1 night in the...       POS\n",
       "2350      40  This is a very nice boutique hotel.  Staff is ...       POS\n",
       "2351      50  Lots to do - pool, beer pong, volcano boarding...       POS\n",
       "\n",
       "[2352 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d863f059",
   "metadata": {},
   "source": [
    "## Ejemplo modelo simple\n",
    "Se entrena un modelo simple para hacer pruebas del flujo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971b8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-04 02:22:00.199936 POS class start\n",
      "2022-07-04 02:22:00.200055   sentences_as_words\n",
      "2022-07-04 02:22:03.991336   remove_stopwords\n",
      "2022-07-04 02:22:04.193294   ngram\n",
      "2022-07-04 02:22:07.686242 NEG class start\n",
      "2022-07-04 02:22:07.686469   sentences_as_words\n",
      "2022-07-04 02:22:08.329028   remove_stopwords\n",
      "2022-07-04 02:22:08.363317   ngram\n",
      "2022-07-04 02:22:09.006042 probs\n",
      "2022-07-04 02:22:09.723569 end\n"
     ]
    }
   ],
   "source": [
    "model_test = Model(\n",
    "    df = train\n",
    "    , steps = ['remove_stopwords','ngram']\n",
    "    , nlp_model = 'en_core_web_sm'\n",
    "    , ngrams = 2\n",
    "    , min_count = 5\n",
    "    , threshold = 10\n",
    "    , allowed_postags = ALLOWED_POSTAGS\n",
    "    , debug = False\n",
    ")\n",
    "model_test.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df55b59c",
   "metadata": {},
   "source": [
    "**Sanity check** de un caso negativo y uno positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09dc5297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -21.14690692178164, 'probs': [-3.915511649867557, -9.478052593473269, -7.623772767263879, -0.1295699111769321]}, 'NEG': {'prob': -19.56974651206995, 'probs': [-4.28513024399396, -6.872302738240207, -6.304693302624039, -2.1076202272117475]}, 'selected': 'NEG'}\n",
      ">> Selected class NEG\n"
     ]
    }
   ],
   "source": [
    "result = model_test.predict(\"the hotel was dirty and noisy\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0ebe89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -15.605211573416558, 'probs': [-3.915511649867557, -4.785817268499889, -6.77431274387218, -0.1295699111769321]}, 'NEG': {'prob': -20.27825475622301, 'probs': [-4.28513024399396, -5.520297450598949, -8.365206834418355, -2.1076202272117475]}, 'selected': 'POS'}\n",
      ">> Selected class POS\n"
     ]
    }
   ],
   "source": [
    "result = model_test.predict(\"the hotel was very clean and I love it! :)\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c459",
   "metadata": {},
   "source": [
    "Probar con la data de validación para **obtener las métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3461fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8410 (1978)\n",
      "      FP Rate: 0.0685 (161)\n",
      "      TN Rate: 0.0663 (156)\n",
      "      FN Rate: 0.0242 (57)\n",
      "    Accuracy: 0.9073\n",
      "    Precision: 0.9247\n",
      "    Recall: 0.9720\n",
      "    F1: 0.9478\n"
     ]
    }
   ],
   "source": [
    "model_metrics = get_metrics(model=model_test, df=validation)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81159bed",
   "metadata": {},
   "source": [
    "## Definir todos los modelos a entrenar\n",
    "Se creó un dataframe para incluir todos los modelos con los que se desea entrenar para hacer pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9278774",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_train = []\n",
    "\n",
    "# ###################################\n",
    "# solo stopwords\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# solo ngrams\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 10\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 2, 'min_count': 10, 'threshold': 50\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 3, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['ngram']\n",
    "    , 'ngrams': 4, 'min_count': 7, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# stopwords y bigramas\n",
    "# variando min_count y threshold\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 1, 'threshold': 5\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 3, 'threshold': 5\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 10\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 30\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 15, 'threshold': 30\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# modelos con stopwords y trigramas+\n",
    "# variando min_count y threshold\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 4, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 4, 'min_count': 2, 'threshold': 5\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 15, 'threshold': 30\n",
    "    , 'nlp_model': None, 'allowed_postags': None\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# solo lematizando\n",
    "# variando el modelo de spacy\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_sm', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_md', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_trf', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# lematizando y stopwords\n",
    "# variando el modelo de spacy\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization']\n",
    "    , 'ngrams': None, 'min_count': None, 'threshold': None\n",
    "    , 'nlp_model': 'en_core_web_trf', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "\n",
    "# ###################################\n",
    "# lematizacion y con ngrams\n",
    "# variando el modelo de spacy\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_sm', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_md', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 5, 'threshold': 20\n",
    "    , 'nlp_model': 'en_core_web_trf', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 5, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 3, 'min_count': 7, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ALLOWED_POSTAGS\n",
    "})\n",
    "\n",
    "# ###################################\n",
    "# lematizacion y con ngrams\n",
    "# variando el listado de postags\n",
    "# anteriormente se habia agregado por\n",
    "# default 'PART', por lo que se prueba\n",
    "# quitarlo\n",
    "# ###################################\n",
    "models_to_train.append({\n",
    "    'steps': ['remove_stopwords','lemmatization','ngram']\n",
    "    , 'ngrams': 2, 'min_count': 7, 'threshold': 25\n",
    "    , 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96af872",
   "metadata": {},
   "source": [
    "## Entrenar todos los modelos\n",
    "Se obtienen todas las métricas de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8f6cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************\n",
      "Start training model 1/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:22:13.191574 POS class start\n",
      "2022-07-04 02:22:13.191661   sentences_as_words\n",
      "2022-07-04 02:22:16.967081   remove_stopwords\n",
      "2022-07-04 02:22:17.389771 NEG class start\n",
      "2022-07-04 02:22:17.390568   sentences_as_words\n",
      "2022-07-04 02:22:18.037089   remove_stopwords\n",
      "2022-07-04 02:22:18.073900 probs\n",
      "2022-07-04 02:22:18.795371 end\n",
      "Training time: 0.09 min\n",
      "\n",
      "2022-07-04 02:22:18.814349 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8282 (1948)\n",
      "      FP Rate: 0.0514 (121)\n",
      "      TN Rate: 0.0833 (196)\n",
      "      FN Rate: 0.0370 (87)\n",
      "    Accuracy: 0.9116\n",
      "    Precision: 0.9415\n",
      "    Recall: 0.9572\n",
      "    F1: 0.9493\n",
      "Evaluation time: 2.87 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 2/30\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 2, 'min_count': 7, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:22:21.696023 POS class start\n",
      "2022-07-04 02:22:21.696126   sentences_as_words\n",
      "2022-07-04 02:22:25.480673   ngram\n",
      "2022-07-04 02:22:31.090027 NEG class start\n",
      "2022-07-04 02:22:31.090410   sentences_as_words\n",
      "2022-07-04 02:22:31.751069   ngram\n",
      "2022-07-04 02:22:32.842589 probs\n",
      "2022-07-04 02:22:33.928780 end\n",
      "Training time: 0.20 min\n",
      "\n",
      "2022-07-04 02:22:33.956961 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8261 (1943)\n",
      "      FP Rate: 0.0570 (134)\n",
      "      TN Rate: 0.0778 (183)\n",
      "      FN Rate: 0.0391 (92)\n",
      "    Accuracy: 0.9039\n",
      "    Precision: 0.9355\n",
      "    Recall: 0.9548\n",
      "    F1: 0.9450\n",
      "Evaluation time: 3.76 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 3/30\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 10, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:22:37.732070 POS class start\n",
      "2022-07-04 02:22:37.732168   sentences_as_words\n",
      "2022-07-04 02:22:41.549351   ngram\n",
      "2022-07-04 02:22:47.106084 NEG class start\n",
      "2022-07-04 02:22:47.106326   sentences_as_words\n",
      "2022-07-04 02:22:47.770686   ngram\n",
      "2022-07-04 02:22:48.838152 probs\n",
      "2022-07-04 02:22:49.972086 end\n",
      "Training time: 0.20 min\n",
      "\n",
      "2022-07-04 02:22:50.003657 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8189 (1926)\n",
      "      FP Rate: 0.0527 (124)\n",
      "      TN Rate: 0.0821 (193)\n",
      "      FN Rate: 0.0463 (109)\n",
      "    Accuracy: 0.9009\n",
      "    Precision: 0.9395\n",
      "    Recall: 0.9464\n",
      "    F1: 0.9430\n",
      "Evaluation time: 3.77 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 4/30\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 2, 'min_count': 10, 'threshold': 50, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:22:53.779291 POS class start\n",
      "2022-07-04 02:22:53.779382   sentences_as_words\n",
      "2022-07-04 02:22:57.562599   ngram\n",
      "2022-07-04 02:23:03.220155 NEG class start\n",
      "2022-07-04 02:23:03.220972   sentences_as_words\n",
      "2022-07-04 02:23:03.881763   ngram\n",
      "2022-07-04 02:23:04.959699 probs\n",
      "2022-07-04 02:23:06.058314 end\n",
      "Training time: 0.21 min\n",
      "\n",
      "2022-07-04 02:23:06.088079 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8316 (1956)\n",
      "      FP Rate: 0.0595 (140)\n",
      "      TN Rate: 0.0753 (177)\n",
      "      FN Rate: 0.0336 (79)\n",
      "    Accuracy: 0.9069\n",
      "    Precision: 0.9332\n",
      "    Recall: 0.9612\n",
      "    F1: 0.9470\n",
      "Evaluation time: 3.79 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 5/30\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 3, 'min_count': 7, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:23:09.882720 POS class start\n",
      "2022-07-04 02:23:09.882821   sentences_as_words\n",
      "2022-07-04 02:23:13.666315   ngram\n",
      "2022-07-04 02:23:25.100310 NEG class start\n",
      "2022-07-04 02:23:25.100553   sentences_as_words\n",
      "2022-07-04 02:23:25.764267   ngram\n",
      "2022-07-04 02:23:27.899852 probs\n",
      "2022-07-04 02:23:29.006368 end\n",
      "Training time: 0.32 min\n",
      "\n",
      "2022-07-04 02:23:29.038539 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8291 (1950)\n",
      "      FP Rate: 0.0612 (144)\n",
      "      TN Rate: 0.0736 (173)\n",
      "      FN Rate: 0.0361 (85)\n",
      "    Accuracy: 0.9026\n",
      "    Precision: 0.9312\n",
      "    Recall: 0.9582\n",
      "    F1: 0.9445\n",
      "Evaluation time: 4.49 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 6/30\n",
      "*******************************************\n",
      "{'steps': ['ngram'], 'ngrams': 4, 'min_count': 7, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:23:33.533478 POS class start\n",
      "2022-07-04 02:23:33.533565   sentences_as_words\n",
      "2022-07-04 02:23:37.326409   ngram\n",
      "2022-07-04 02:23:53.888790 NEG class start\n",
      "2022-07-04 02:23:53.889031   sentences_as_words\n",
      "2022-07-04 02:23:54.571802   ngram\n",
      "2022-07-04 02:23:57.730702 probs\n",
      "2022-07-04 02:23:58.828502 end\n",
      "Training time: 0.42 min\n",
      "\n",
      "2022-07-04 02:23:58.861747 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8346 (1963)\n",
      "      FP Rate: 0.0629 (148)\n",
      "      TN Rate: 0.0719 (169)\n",
      "      FN Rate: 0.0306 (72)\n",
      "    Accuracy: 0.9065\n",
      "    Precision: 0.9299\n",
      "    Recall: 0.9646\n",
      "    F1: 0.9469\n",
      "Evaluation time: 5.20 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 7/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 1, 'threshold': 5, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:24:04.075139 POS class start\n",
      "2022-07-04 02:24:04.075232   sentences_as_words\n",
      "2022-07-04 02:24:08.045970   remove_stopwords\n",
      "2022-07-04 02:24:08.285106   ngram\n",
      "2022-07-04 02:24:11.783720 NEG class start\n",
      "2022-07-04 02:24:11.783929   sentences_as_words\n",
      "2022-07-04 02:24:12.426818   remove_stopwords\n",
      "2022-07-04 02:24:12.465070   ngram\n",
      "2022-07-04 02:24:13.089114 probs\n",
      "2022-07-04 02:24:13.997866 end\n",
      "Training time: 0.17 min\n",
      "\n",
      "2022-07-04 02:24:14.016605 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8503 (2000)\n",
      "      FP Rate: 0.0812 (191)\n",
      "      TN Rate: 0.0536 (126)\n",
      "      FN Rate: 0.0149 (35)\n",
      "    Accuracy: 0.9039\n",
      "    Precision: 0.9128\n",
      "    Recall: 0.9828\n",
      "    F1: 0.9465\n",
      "Evaluation time: 3.32 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 8/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 3, 'threshold': 5, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:24:17.349800 POS class start\n",
      "2022-07-04 02:24:17.349884   sentences_as_words\n",
      "2022-07-04 02:24:21.119081   remove_stopwords\n",
      "2022-07-04 02:24:21.348430   ngram\n",
      "2022-07-04 02:24:24.813346 NEG class start\n",
      "2022-07-04 02:24:24.813566   sentences_as_words\n",
      "2022-07-04 02:24:25.468939   remove_stopwords\n",
      "2022-07-04 02:24:25.508453   ngram\n",
      "2022-07-04 02:24:26.151390 probs\n",
      "2022-07-04 02:24:26.910914 end\n",
      "Training time: 0.16 min\n",
      "\n",
      "2022-07-04 02:24:26.931279 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8435 (1984)\n",
      "      FP Rate: 0.0723 (170)\n",
      "      TN Rate: 0.0625 (147)\n",
      "      FN Rate: 0.0217 (51)\n",
      "    Accuracy: 0.9060\n",
      "    Precision: 0.9211\n",
      "    Recall: 0.9749\n",
      "    F1: 0.9472\n",
      "Evaluation time: 3.34 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 9/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 10, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:24:30.278838 POS class start\n",
      "2022-07-04 02:24:30.278929   sentences_as_words\n",
      "2022-07-04 02:24:34.049806   remove_stopwords\n",
      "2022-07-04 02:24:34.508810   ngram\n",
      "2022-07-04 02:24:38.014484 NEG class start\n",
      "2022-07-04 02:24:38.014698   sentences_as_words\n",
      "2022-07-04 02:24:38.674596   remove_stopwords\n",
      "2022-07-04 02:24:38.711626   ngram\n",
      "2022-07-04 02:24:39.352678 probs\n",
      "2022-07-04 02:24:40.090656 end\n",
      "Training time: 0.16 min\n",
      "\n",
      "2022-07-04 02:24:40.111728 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8410 (1978)\n",
      "      FP Rate: 0.0685 (161)\n",
      "      TN Rate: 0.0663 (156)\n",
      "      FN Rate: 0.0242 (57)\n",
      "    Accuracy: 0.9073\n",
      "    Precision: 0.9247\n",
      "    Recall: 0.9720\n",
      "    F1: 0.9478\n",
      "Evaluation time: 3.37 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 10/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:24:43.492202 POS class start\n",
      "2022-07-04 02:24:43.492311   sentences_as_words\n",
      "2022-07-04 02:24:47.251781   remove_stopwords\n",
      "2022-07-04 02:24:47.507534   ngram\n",
      "2022-07-04 02:24:51.104697 NEG class start\n",
      "2022-07-04 02:24:51.104930   sentences_as_words\n",
      "2022-07-04 02:24:51.752374   remove_stopwords\n",
      "2022-07-04 02:24:51.790227   ngram\n",
      "2022-07-04 02:24:52.437680 probs\n",
      "2022-07-04 02:24:53.194321 end\n",
      "Training time: 0.16 min\n",
      "\n",
      "2022-07-04 02:24:53.216054 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8393 (1974)\n",
      "      FP Rate: 0.0642 (151)\n",
      "      TN Rate: 0.0706 (166)\n",
      "      FN Rate: 0.0259 (61)\n",
      "    Accuracy: 0.9099\n",
      "    Precision: 0.9289\n",
      "    Recall: 0.9700\n",
      "    F1: 0.9490\n",
      "Evaluation time: 3.38 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 11/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 7, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:24:56.605617 POS class start\n",
      "2022-07-04 02:24:56.606081   sentences_as_words\n",
      "2022-07-04 02:25:00.414327   remove_stopwords\n",
      "2022-07-04 02:25:00.651674   ngram\n",
      "2022-07-04 02:25:04.449147 NEG class start\n",
      "2022-07-04 02:25:04.449544   sentences_as_words\n",
      "2022-07-04 02:25:05.103382   remove_stopwords\n",
      "2022-07-04 02:25:05.141529   ngram\n",
      "2022-07-04 02:25:05.787254 probs\n",
      "2022-07-04 02:25:06.546725 end\n",
      "Training time: 0.17 min\n",
      "\n",
      "2022-07-04 02:25:06.568703 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8359 (1966)\n",
      "      FP Rate: 0.0604 (142)\n",
      "      TN Rate: 0.0744 (175)\n",
      "      FN Rate: 0.0293 (69)\n",
      "    Accuracy: 0.9103\n",
      "    Precision: 0.9326\n",
      "    Recall: 0.9661\n",
      "    F1: 0.9491\n",
      "Evaluation time: 3.45 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 12/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 2, 'min_count': 15, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:25:10.028146 POS class start\n",
      "2022-07-04 02:25:10.028239   sentences_as_words\n",
      "2022-07-04 02:25:13.838454   remove_stopwords\n",
      "2022-07-04 02:25:14.079030   ngram\n",
      "2022-07-04 02:25:17.658135 NEG class start\n",
      "2022-07-04 02:25:17.658364   sentences_as_words\n",
      "2022-07-04 02:25:18.311663   remove_stopwords\n",
      "2022-07-04 02:25:18.368660   ngram\n",
      "2022-07-04 02:25:19.018962 probs\n",
      "2022-07-04 02:25:19.785602 end\n",
      "Training time: 0.16 min\n",
      "\n",
      "2022-07-04 02:25:19.809038 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8363 (1967)\n",
      "      FP Rate: 0.0651 (153)\n",
      "      TN Rate: 0.0697 (164)\n",
      "      FN Rate: 0.0289 (68)\n",
      "    Accuracy: 0.9060\n",
      "    Precision: 0.9278\n",
      "    Recall: 0.9666\n",
      "    F1: 0.9468\n",
      "Evaluation time: 3.41 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 13/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 3, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:25:23.229206 POS class start\n",
      "2022-07-04 02:25:23.229356   sentences_as_words\n",
      "2022-07-04 02:25:26.997188   remove_stopwords\n",
      "2022-07-04 02:25:27.252939   ngram\n",
      "2022-07-04 02:25:34.568805 NEG class start\n",
      "2022-07-04 02:25:34.569074   sentences_as_words\n",
      "2022-07-04 02:25:35.217327   remove_stopwords\n",
      "2022-07-04 02:25:35.254784   ngram\n",
      "2022-07-04 02:25:36.562042 probs\n",
      "2022-07-04 02:25:37.303405 end\n",
      "Training time: 0.24 min\n",
      "\n",
      "2022-07-04 02:25:37.324801 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8448 (1987)\n",
      "      FP Rate: 0.0710 (167)\n",
      "      TN Rate: 0.0638 (150)\n",
      "      FN Rate: 0.0204 (48)\n",
      "    Accuracy: 0.9086\n",
      "    Precision: 0.9225\n",
      "    Recall: 0.9764\n",
      "    F1: 0.9487\n",
      "Evaluation time: 3.78 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 14/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 4, 'min_count': 5, 'threshold': 20, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:25:41.116878 POS class start\n",
      "2022-07-04 02:25:41.116976   sentences_as_words\n",
      "2022-07-04 02:25:44.899979   remove_stopwords\n",
      "2022-07-04 02:25:45.138666   ngram\n",
      "2022-07-04 02:25:55.875278 NEG class start\n",
      "2022-07-04 02:25:55.875505   sentences_as_words\n",
      "2022-07-04 02:25:56.526270   remove_stopwords\n",
      "2022-07-04 02:25:56.565267   ngram\n",
      "2022-07-04 02:25:58.539583 probs\n",
      "2022-07-04 02:25:59.294773 end\n",
      "Training time: 0.30 min\n",
      "\n",
      "2022-07-04 02:25:59.317388 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8448 (1987)\n",
      "      FP Rate: 0.0710 (167)\n",
      "      TN Rate: 0.0638 (150)\n",
      "      FN Rate: 0.0204 (48)\n",
      "    Accuracy: 0.9086\n",
      "    Precision: 0.9225\n",
      "    Recall: 0.9764\n",
      "    F1: 0.9487\n",
      "Evaluation time: 4.24 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 15/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 4, 'min_count': 2, 'threshold': 5, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:26:03.563929 POS class start\n",
      "2022-07-04 02:26:03.564029   sentences_as_words\n",
      "2022-07-04 02:26:07.336895   remove_stopwords\n",
      "2022-07-04 02:26:07.788153   ngram\n",
      "2022-07-04 02:26:18.174611 NEG class start\n",
      "2022-07-04 02:26:18.174856   sentences_as_words\n",
      "2022-07-04 02:26:18.820938   remove_stopwords\n",
      "2022-07-04 02:26:18.860704   ngram\n",
      "2022-07-04 02:26:20.770384 probs\n",
      "2022-07-04 02:26:21.604725 end\n",
      "Training time: 0.30 min\n",
      "\n",
      "2022-07-04 02:26:21.624801 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8516 (2003)\n",
      "      FP Rate: 0.0846 (199)\n",
      "      TN Rate: 0.0502 (118)\n",
      "      FN Rate: 0.0136 (32)\n",
      "    Accuracy: 0.9018\n",
      "    Precision: 0.9096\n",
      "    Recall: 0.9843\n",
      "    F1: 0.9455\n",
      "Evaluation time: 4.12 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 16/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'ngram'], 'ngrams': 3, 'min_count': 15, 'threshold': 30, 'nlp_model': None, 'allowed_postags': None}\n",
      "\n",
      "2022-07-04 02:26:25.757454 POS class start\n",
      "2022-07-04 02:26:25.757551   sentences_as_words\n",
      "2022-07-04 02:26:29.532791   remove_stopwords\n",
      "2022-07-04 02:26:29.769011   ngram\n",
      "2022-07-04 02:26:37.157936 NEG class start\n",
      "2022-07-04 02:26:37.158153   sentences_as_words\n",
      "2022-07-04 02:26:37.807233   remove_stopwords\n",
      "2022-07-04 02:26:37.847944   ngram\n",
      "2022-07-04 02:26:39.161855 probs\n",
      "2022-07-04 02:26:39.917358 end\n",
      "Training time: 0.24 min\n",
      "\n",
      "2022-07-04 02:26:39.939943 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8372 (1969)\n",
      "      FP Rate: 0.0663 (156)\n",
      "      TN Rate: 0.0685 (161)\n",
      "      FN Rate: 0.0281 (66)\n",
      "    Accuracy: 0.9056\n",
      "    Precision: 0.9266\n",
      "    Recall: 0.9676\n",
      "    F1: 0.9466\n",
      "Evaluation time: 3.86 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 17/30\n",
      "*******************************************\n",
      "{'steps': ['lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_sm', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 02:26:43.810830 POS class start\n",
      "2022-07-04 02:26:43.810926   sentences_as_words\n",
      "2022-07-04 02:26:47.592717   lemmatization\n",
      "2022-07-04 02:30:25.946022 NEG class start\n",
      "2022-07-04 02:30:25.947660   sentences_as_words\n",
      "2022-07-04 02:30:26.600212   lemmatization\n",
      "2022-07-04 02:31:01.305033 probs\n",
      "2022-07-04 02:31:01.947007 end\n",
      "Training time: 4.30 min\n",
      "\n",
      "2022-07-04 02:31:01.966021 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8236 (1937)\n",
      "      FP Rate: 0.0476 (112)\n",
      "      TN Rate: 0.0872 (205)\n",
      "      FN Rate: 0.0417 (98)\n",
      "    Accuracy: 0.9107\n",
      "    Precision: 0.9453\n",
      "    Recall: 0.9518\n",
      "    F1: 0.9486\n",
      "Evaluation time: 110.26 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 18/30\n",
      "*******************************************\n",
      "{'steps': ['lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_md', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 02:32:52.241174 POS class start\n",
      "2022-07-04 02:32:52.241272   sentences_as_words\n",
      "2022-07-04 02:32:56.063268   lemmatization\n",
      "2022-07-04 02:36:53.618790 NEG class start\n",
      "2022-07-04 02:36:53.619256   sentences_as_words\n",
      "2022-07-04 02:36:54.269044   lemmatization\n",
      "2022-07-04 02:37:32.040754 probs\n",
      "2022-07-04 02:37:32.686352 end\n",
      "Training time: 4.67 min\n",
      "\n",
      "2022-07-04 02:37:32.704721 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8240 (1938)\n",
      "      FP Rate: 0.0434 (102)\n",
      "      TN Rate: 0.0914 (215)\n",
      "      FN Rate: 0.0412 (97)\n",
      "    Accuracy: 0.9154\n",
      "    Precision: 0.9500\n",
      "    Recall: 0.9523\n",
      "    F1: 0.9512\n",
      "Evaluation time: 121.87 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 19/30\n",
      "*******************************************\n",
      "{'steps': ['lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 02:39:34.584939 POS class start\n",
      "2022-07-04 02:39:34.585030   sentences_as_words\n",
      "2022-07-04 02:39:38.387524   lemmatization\n",
      "2022-07-04 02:43:34.224739 NEG class start\n",
      "2022-07-04 02:43:34.225192   sentences_as_words\n",
      "2022-07-04 02:43:34.884659   lemmatization\n",
      "2022-07-04 02:44:12.445879 probs\n",
      "2022-07-04 02:44:13.092432 end\n",
      "Training time: 4.64 min\n",
      "\n",
      "2022-07-04 02:44:13.112143 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8291 (1950)\n",
      "      FP Rate: 0.0434 (102)\n",
      "      TN Rate: 0.0914 (215)\n",
      "      FN Rate: 0.0361 (85)\n",
      "    Accuracy: 0.9205\n",
      "    Precision: 0.9503\n",
      "    Recall: 0.9582\n",
      "    F1: 0.9542\n",
      "Evaluation time: 118.52 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 20/30\n",
      "*******************************************\n",
      "{'steps': ['lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_trf', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 02:46:11.645781 POS class start\n",
      "2022-07-04 02:46:11.645879   sentences_as_words\n",
      "2022-07-04 02:46:15.451788   lemmatization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/kegarcia/Documents/maestria/cmiads-tmnlp-h3/venv/lib64/python3.6/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-04 03:12:49.474444 NEG class start\n",
      "2022-07-04 03:12:49.474910   sentences_as_words\n",
      "2022-07-04 03:12:50.146339   lemmatization\n",
      "2022-07-04 03:17:11.222597 probs\n",
      "2022-07-04 03:17:11.888118 end\n",
      "Training time: 31.00 min\n",
      "\n",
      "2022-07-04 03:17:11.911661 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8265 (1944)\n",
      "      FP Rate: 0.0434 (102)\n",
      "      TN Rate: 0.0914 (215)\n",
      "      FN Rate: 0.0387 (91)\n",
      "    Accuracy: 0.9179\n",
      "    Precision: 0.9501\n",
      "    Recall: 0.9553\n",
      "    F1: 0.9527\n",
      "Evaluation time: 818.09 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 21/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 03:30:50.008580 POS class start\n",
      "2022-07-04 03:30:50.008667   sentences_as_words\n",
      "2022-07-04 03:30:53.938039   remove_stopwords\n",
      "2022-07-04 03:30:54.223152   lemmatization\n",
      "2022-07-04 03:33:38.238732 NEG class start\n",
      "2022-07-04 03:33:38.239273   sentences_as_words\n",
      "2022-07-04 03:33:38.910652   remove_stopwords\n",
      "2022-07-04 03:33:38.949372   lemmatization\n",
      "2022-07-04 03:34:04.132403 probs\n",
      "2022-07-04 03:34:04.748747 end\n",
      "Training time: 3.25 min\n",
      "\n",
      "2022-07-04 03:34:04.770824 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8312 (1955)\n",
      "      FP Rate: 0.0497 (117)\n",
      "      TN Rate: 0.0850 (200)\n",
      "      FN Rate: 0.0340 (80)\n",
      "    Accuracy: 0.9162\n",
      "    Precision: 0.9435\n",
      "    Recall: 0.9607\n",
      "    F1: 0.9520\n",
      "Evaluation time: 84.31 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 22/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization'], 'ngrams': None, 'min_count': None, 'threshold': None, 'nlp_model': 'en_core_web_trf', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 03:35:29.089193 POS class start\n",
      "2022-07-04 03:35:29.089298   sentences_as_words\n",
      "2022-07-04 03:35:32.954034   remove_stopwords\n",
      "2022-07-04 03:35:33.191295   lemmatization\n",
      "2022-07-04 03:53:26.620754 NEG class start\n",
      "2022-07-04 03:53:26.621179   sentences_as_words\n",
      "2022-07-04 03:53:27.305372   remove_stopwords\n",
      "2022-07-04 03:53:27.347082   lemmatization\n",
      "2022-07-04 03:56:03.735845 probs\n",
      "2022-07-04 03:56:04.373676 end\n",
      "Training time: 20.59 min\n",
      "\n",
      "2022-07-04 03:56:04.403102 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8278 (1947)\n",
      "      FP Rate: 0.0497 (117)\n",
      "      TN Rate: 0.0850 (200)\n",
      "      FN Rate: 0.0374 (88)\n",
      "    Accuracy: 0.9128\n",
      "    Precision: 0.9433\n",
      "    Recall: 0.9568\n",
      "    F1: 0.9500\n",
      "Evaluation time: 511.52 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 23/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': 'en_core_web_sm', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:04:35.942253 POS class start\n",
      "2022-07-04 04:04:35.942377   sentences_as_words\n",
      "2022-07-04 04:04:39.847420   remove_stopwords\n",
      "2022-07-04 04:04:40.434360   lemmatization\n",
      "2022-07-04 04:07:12.193601   ngram\n",
      "2022-07-04 04:07:15.293085 NEG class start\n",
      "2022-07-04 04:07:15.293417   sentences_as_words\n",
      "2022-07-04 04:07:15.966090   remove_stopwords\n",
      "2022-07-04 04:07:16.034380   lemmatization\n",
      "2022-07-04 04:07:38.724267   ngram\n",
      "2022-07-04 04:07:39.308962 probs\n",
      "2022-07-04 04:07:39.966478 end\n",
      "Training time: 3.07 min\n",
      "\n",
      "2022-07-04 04:07:39.993586 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8312 (1955)\n",
      "      FP Rate: 0.0536 (126)\n",
      "      TN Rate: 0.0812 (191)\n",
      "      FN Rate: 0.0340 (80)\n",
      "    Accuracy: 0.9124\n",
      "    Precision: 0.9395\n",
      "    Recall: 0.9607\n",
      "    F1: 0.9500\n",
      "Evaluation time: 78.33 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 24/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': 'en_core_web_md', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:08:58.336835 POS class start\n",
      "2022-07-04 04:08:58.336980   sentences_as_words\n",
      "2022-07-04 04:09:02.230888   remove_stopwords\n",
      "2022-07-04 04:09:02.497836   lemmatization\n",
      "2022-07-04 04:11:48.919300   ngram\n",
      "2022-07-04 04:11:52.021639 NEG class start\n",
      "2022-07-04 04:11:52.021940   sentences_as_words\n",
      "2022-07-04 04:11:52.688743   remove_stopwords\n",
      "2022-07-04 04:11:52.732958   lemmatization\n",
      "2022-07-04 04:12:17.732517   ngram\n",
      "2022-07-04 04:12:18.308695 probs\n",
      "2022-07-04 04:12:18.919221 end\n",
      "Training time: 3.34 min\n",
      "\n",
      "2022-07-04 04:12:18.943033 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8316 (1956)\n",
      "      FP Rate: 0.0587 (138)\n",
      "      TN Rate: 0.0761 (179)\n",
      "      FN Rate: 0.0336 (79)\n",
      "    Accuracy: 0.9077\n",
      "    Precision: 0.9341\n",
      "    Recall: 0.9612\n",
      "    F1: 0.9474\n",
      "Evaluation time: 85.63 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 25/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:13:44.590187 POS class start\n",
      "2022-07-04 04:13:44.590350   sentences_as_words\n",
      "2022-07-04 04:13:48.480997   remove_stopwords\n",
      "2022-07-04 04:13:48.745401   lemmatization\n",
      "2022-07-04 04:16:33.165126   ngram\n",
      "2022-07-04 04:16:36.211499 NEG class start\n",
      "2022-07-04 04:16:36.211821   sentences_as_words\n",
      "2022-07-04 04:16:36.874921   remove_stopwords\n",
      "2022-07-04 04:16:36.916583   lemmatization\n",
      "2022-07-04 04:17:01.449505   ngram\n",
      "2022-07-04 04:17:02.007540 probs\n",
      "2022-07-04 04:17:02.603840 end\n",
      "Training time: 3.30 min\n",
      "\n",
      "2022-07-04 04:17:02.624958 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8325 (1958)\n",
      "      FP Rate: 0.0582 (137)\n",
      "      TN Rate: 0.0765 (180)\n",
      "      FN Rate: 0.0327 (77)\n",
      "    Accuracy: 0.9090\n",
      "    Precision: 0.9346\n",
      "    Recall: 0.9622\n",
      "    F1: 0.9482\n",
      "Evaluation time: 84.68 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 26/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 2, 'min_count': 5, 'threshold': 20, 'nlp_model': 'en_core_web_trf', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:18:27.320385 POS class start\n",
      "2022-07-04 04:18:27.320484   sentences_as_words\n",
      "2022-07-04 04:18:31.187838   remove_stopwords\n",
      "2022-07-04 04:18:31.469547   lemmatization\n",
      "2022-07-04 04:35:44.113200   ngram\n",
      "2022-07-04 04:35:47.141634 NEG class start\n",
      "2022-07-04 04:35:47.141969   sentences_as_words\n",
      "2022-07-04 04:35:47.848091   remove_stopwords\n",
      "2022-07-04 04:35:47.890685   lemmatization\n",
      "2022-07-04 04:39:05.224149   ngram\n",
      "2022-07-04 04:39:05.792240 probs\n",
      "2022-07-04 04:39:06.400969 end\n",
      "Training time: 20.65 min\n",
      "\n",
      "2022-07-04 04:39:06.422708 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8291 (1950)\n",
      "      FP Rate: 0.0570 (134)\n",
      "      TN Rate: 0.0778 (183)\n",
      "      FN Rate: 0.0361 (85)\n",
      "    Accuracy: 0.9069\n",
      "    Precision: 0.9357\n",
      "    Recall: 0.9582\n",
      "    F1: 0.9468\n",
      "Evaluation time: 511.11 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 27/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 3, 'min_count': 5, 'threshold': 25, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:47:37.541781 POS class start\n",
      "2022-07-04 04:47:37.541883   sentences_as_words\n",
      "2022-07-04 04:47:41.483778   remove_stopwords\n",
      "2022-07-04 04:47:41.782022   lemmatization\n",
      "2022-07-04 04:50:26.621685   ngram\n",
      "2022-07-04 04:50:32.888830 NEG class start\n",
      "2022-07-04 04:50:32.889059   sentences_as_words\n",
      "2022-07-04 04:50:33.554296   remove_stopwords\n",
      "2022-07-04 04:50:33.596740   lemmatization\n",
      "2022-07-04 04:50:58.002122   ngram\n",
      "2022-07-04 04:50:59.103735 probs\n",
      "2022-07-04 04:50:59.720826 end\n",
      "Training time: 3.37 min\n",
      "\n",
      "2022-07-04 04:50:59.743938 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8338 (1961)\n",
      "      FP Rate: 0.0591 (139)\n",
      "      TN Rate: 0.0757 (178)\n",
      "      FN Rate: 0.0315 (74)\n",
      "    Accuracy: 0.9094\n",
      "    Precision: 0.9338\n",
      "    Recall: 0.9636\n",
      "    F1: 0.9485\n",
      "Evaluation time: 84.99 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 28/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 3, 'min_count': 5, 'threshold': 25, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:52:24.748073 POS class start\n",
      "2022-07-04 04:52:24.748171   sentences_as_words\n",
      "2022-07-04 04:52:28.658192   remove_stopwords\n",
      "2022-07-04 04:52:28.914583   lemmatization\n",
      "2022-07-04 04:55:11.930262   ngram\n",
      "2022-07-04 04:55:17.928462 NEG class start\n",
      "2022-07-04 04:55:17.929494   sentences_as_words\n",
      "2022-07-04 04:55:18.594046   remove_stopwords\n",
      "2022-07-04 04:55:18.636148   lemmatization\n",
      "2022-07-04 04:55:43.070260   ngram\n",
      "2022-07-04 04:55:44.178313 probs\n",
      "2022-07-04 04:55:44.811989 end\n",
      "Training time: 3.34 min\n",
      "\n",
      "2022-07-04 04:55:44.841462 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8338 (1961)\n",
      "      FP Rate: 0.0591 (139)\n",
      "      TN Rate: 0.0757 (178)\n",
      "      FN Rate: 0.0315 (74)\n",
      "    Accuracy: 0.9094\n",
      "    Precision: 0.9338\n",
      "    Recall: 0.9636\n",
      "    F1: 0.9485\n",
      "Evaluation time: 85.00 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 29/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 3, 'min_count': 7, 'threshold': 25, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV', 'PART']}\n",
      "\n",
      "2022-07-04 04:57:09.847717 POS class start\n",
      "2022-07-04 04:57:09.847814   sentences_as_words\n",
      "2022-07-04 04:57:13.744314   remove_stopwords\n",
      "2022-07-04 04:57:13.994540   lemmatization\n",
      "2022-07-04 04:59:57.196275   ngram\n",
      "2022-07-04 05:00:03.283458 NEG class start\n",
      "2022-07-04 05:00:03.283767   sentences_as_words\n",
      "2022-07-04 05:00:03.962924   remove_stopwords\n",
      "2022-07-04 05:00:04.006563   lemmatization\n",
      "2022-07-04 05:00:28.407430   ngram\n",
      "2022-07-04 05:00:29.602408 probs\n",
      "2022-07-04 05:00:30.239868 end\n",
      "Training time: 3.34 min\n",
      "\n",
      "2022-07-04 05:00:30.284017 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8372 (1969)\n",
      "      FP Rate: 0.0608 (143)\n",
      "      TN Rate: 0.0740 (174)\n",
      "      FN Rate: 0.0281 (66)\n",
      "    Accuracy: 0.9111\n",
      "    Precision: 0.9323\n",
      "    Recall: 0.9676\n",
      "    F1: 0.9496\n",
      "Evaluation time: 88.05 seconds\n",
      "\n",
      "*******************************************\n",
      "Start training model 30/30\n",
      "*******************************************\n",
      "{'steps': ['remove_stopwords', 'lemmatization', 'ngram'], 'ngrams': 2, 'min_count': 7, 'threshold': 25, 'nlp_model': 'en_core_web_lg', 'allowed_postags': ['NOUN', 'ADJ', 'VERB', 'ADV']}\n",
      "\n",
      "2022-07-04 05:01:58.348710 POS class start\n",
      "2022-07-04 05:01:58.348835   sentences_as_words\n",
      "2022-07-04 05:02:02.261385   remove_stopwords\n",
      "2022-07-04 05:02:02.974630   lemmatization\n",
      "2022-07-04 05:04:47.520910   ngram\n",
      "2022-07-04 05:04:50.562777 NEG class start\n",
      "2022-07-04 05:04:50.563243   sentences_as_words\n",
      "2022-07-04 05:04:51.238745   remove_stopwords\n",
      "2022-07-04 05:04:51.284455   lemmatization\n",
      "2022-07-04 05:05:15.937704   ngram\n",
      "2022-07-04 05:05:16.505468 probs\n",
      "2022-07-04 05:05:17.101816 end\n",
      "Training time: 3.31 min\n",
      "\n",
      "2022-07-04 05:05:17.131028 validation metrics\n",
      "    Evaluados: 2352\n",
      "      TP Rate: 0.8350 (1964)\n",
      "      FP Rate: 0.0565 (133)\n",
      "      TN Rate: 0.0782 (184)\n",
      "      FN Rate: 0.0302 (71)\n",
      "    Accuracy: 0.9133\n",
      "    Precision: 0.9366\n",
      "    Recall: 0.9651\n",
      "    F1: 0.9506\n",
      "Evaluation time: 84.88 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_results = []\n",
    "for model_to_train in models_to_train:\n",
    "    print('*******************************************')\n",
    "    print(f'Start training model {len(model_results)+1}/{len(models_to_train)}')\n",
    "    print('*******************************************')\n",
    "    print(model_to_train)\n",
    "    print('')\n",
    "    \n",
    "    # build model\n",
    "    model = Model(\n",
    "        df = train\n",
    "        , steps = model_to_train['steps']\n",
    "        , nlp_model = model_to_train['nlp_model']\n",
    "        , ngrams = model_to_train['ngrams']\n",
    "        , min_count = model_to_train['min_count']\n",
    "        , threshold = model_to_train['threshold']\n",
    "        , allowed_postags = model_to_train['allowed_postags']\n",
    "        , debug = False\n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    time_start_training = datetime.now()\n",
    "    model.fit()\n",
    "    model_to_train['model'] = model\n",
    "    train_min = (datetime.now()-time_start_training).total_seconds()/60.0\n",
    "    model_to_train['train_min'] = train_min\n",
    "    print('Training time: '+'%.2f'%train_min+' min')\n",
    "    \n",
    "    # test model with validation dataframe\n",
    "    print('')\n",
    "    print(f'{datetime.now()} validation metrics')\n",
    "    time_start_eval = datetime.now()\n",
    "    model_metrics = get_metrics(model=model, df=validation)\n",
    "    eval_sec = (datetime.now()-time_start_eval).total_seconds()\n",
    "    print_metrics(model_metrics)\n",
    "    print('Evaluation time: '+'%.2f'%eval_sec+' seconds')\n",
    "    \n",
    "    # append results\n",
    "    model_results.append({**model_to_train, **model_metrics, 'eval_sec':eval_sec})\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e888f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>train_min</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>eval_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.093824</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.941518</td>\n",
       "      <td>0.957248</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.828231</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>2.873141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945039</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.954791</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.826105</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.039116</td>\n",
       "      <td>3.764681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.204680</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.942962</td>\n",
       "      <td>0.939512</td>\n",
       "      <td>0.946437</td>\n",
       "      <td>0.900935</td>\n",
       "      <td>0.818878</td>\n",
       "      <td>0.082058</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>3.766310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.205255</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946986</td>\n",
       "      <td>0.933206</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>3.785317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.319370</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.944539</td>\n",
       "      <td>0.931232</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.902636</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.073554</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>4.486029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.422243</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946937</td>\n",
       "      <td>0.929891</td>\n",
       "      <td>0.964619</td>\n",
       "      <td>0.906463</td>\n",
       "      <td>0.834609</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>5.204129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.165792</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946522</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>3.324612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.159784</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947243</td>\n",
       "      <td>0.921077</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>3.338723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.164007</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947772</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.971990</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.840986</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.024235</td>\n",
       "      <td>3.371420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949038</td>\n",
       "      <td>0.928941</td>\n",
       "      <td>0.970025</td>\n",
       "      <td>0.909864</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.070578</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>3.380885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.166183</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949071</td>\n",
       "      <td>0.932638</td>\n",
       "      <td>0.966093</td>\n",
       "      <td>0.910289</td>\n",
       "      <td>0.835884</td>\n",
       "      <td>0.074405</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>3.450147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.163125</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946811</td>\n",
       "      <td>0.927830</td>\n",
       "      <td>0.966585</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>3.410430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.235033</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>3.783208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.303449</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>4.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.301127</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945480</td>\n",
       "      <td>0.909628</td>\n",
       "      <td>0.984275</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.851616</td>\n",
       "      <td>0.050170</td>\n",
       "      <td>0.084609</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>4.123538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.236512</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946635</td>\n",
       "      <td>0.926588</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.905612</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>3.861520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.302722</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948580</td>\n",
       "      <td>0.945339</td>\n",
       "      <td>0.951843</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.823554</td>\n",
       "      <td>0.087160</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>110.264073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.674562</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.951166</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.952334</td>\n",
       "      <td>0.915391</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.041241</td>\n",
       "      <td>121.869415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.642231</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.954245</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.920493</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>118.523124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>31.004590</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952708</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.955283</td>\n",
       "      <td>0.917942</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.038690</td>\n",
       "      <td>818.085331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.246148</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952033</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.916241</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>84.307367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>20.588683</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949988</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>0.956757</td>\n",
       "      <td>0.912840</td>\n",
       "      <td>0.827806</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>511.524469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.067748</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949951</td>\n",
       "      <td>0.939452</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.912415</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>78.328459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.343620</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947445</td>\n",
       "      <td>0.934097</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.907738</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>85.630378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.300843</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948184</td>\n",
       "      <td>0.934606</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>0.909014</td>\n",
       "      <td>0.832483</td>\n",
       "      <td>0.076531</td>\n",
       "      <td>0.058248</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>84.684338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>20.651870</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946832</td>\n",
       "      <td>0.935701</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>511.107338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.370207</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.992992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.335058</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.995357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.340769</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.911139</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.073980</td>\n",
       "      <td>0.060799</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>88.051997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV]</td>\n",
       "      <td>3.313224</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>84.878537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       steps  ngrams  min_count  threshold  \\\n",
       "0                         [remove_stopwords]     NaN        NaN        NaN   \n",
       "1                                    [ngram]     2.0        7.0       20.0   \n",
       "2                                    [ngram]     2.0        5.0       10.0   \n",
       "3                                    [ngram]     2.0       10.0       50.0   \n",
       "4                                    [ngram]     3.0        7.0       20.0   \n",
       "5                                    [ngram]     4.0        7.0       20.0   \n",
       "6                  [remove_stopwords, ngram]     2.0        1.0        5.0   \n",
       "7                  [remove_stopwords, ngram]     2.0        3.0        5.0   \n",
       "8                  [remove_stopwords, ngram]     2.0        5.0       10.0   \n",
       "9                  [remove_stopwords, ngram]     2.0        5.0       20.0   \n",
       "10                 [remove_stopwords, ngram]     2.0        7.0       30.0   \n",
       "11                 [remove_stopwords, ngram]     2.0       15.0       30.0   \n",
       "12                 [remove_stopwords, ngram]     3.0        5.0       20.0   \n",
       "13                 [remove_stopwords, ngram]     4.0        5.0       20.0   \n",
       "14                 [remove_stopwords, ngram]     4.0        2.0        5.0   \n",
       "15                 [remove_stopwords, ngram]     3.0       15.0       30.0   \n",
       "16                           [lemmatization]     NaN        NaN        NaN   \n",
       "17                           [lemmatization]     NaN        NaN        NaN   \n",
       "18                           [lemmatization]     NaN        NaN        NaN   \n",
       "19                           [lemmatization]     NaN        NaN        NaN   \n",
       "20         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "21         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "22  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "23  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "24  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "25  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "26  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "27  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "28  [remove_stopwords, lemmatization, ngram]     3.0        7.0       25.0   \n",
       "29  [remove_stopwords, lemmatization, ngram]     2.0        7.0       25.0   \n",
       "\n",
       "          nlp_model               allowed_postags  train_min  evaluated  \\\n",
       "0              None                          None   0.093824       2352   \n",
       "1              None                          None   0.204473       2352   \n",
       "2              None                          None   0.204680       2352   \n",
       "3              None                          None   0.205255       2352   \n",
       "4              None                          None   0.319370       2352   \n",
       "5              None                          None   0.422243       2352   \n",
       "6              None                          None   0.165792       2352   \n",
       "7              None                          None   0.159784       2352   \n",
       "8              None                          None   0.164007       2352   \n",
       "9              None                          None   0.162186       2352   \n",
       "10             None                          None   0.166183       2352   \n",
       "11             None                          None   0.163125       2352   \n",
       "12             None                          None   0.235033       2352   \n",
       "13             None                          None   0.303449       2352   \n",
       "14             None                          None   0.301127       2352   \n",
       "15             None                          None   0.236512       2352   \n",
       "16   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   4.302722       2352   \n",
       "17   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   4.674562       2352   \n",
       "18   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   4.642231       2352   \n",
       "19  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  31.004590       2352   \n",
       "20   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.246148       2352   \n",
       "21  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  20.588683       2352   \n",
       "22   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   3.067748       2352   \n",
       "23   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   3.343620       2352   \n",
       "24   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.300843       2352   \n",
       "25  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  20.651870       2352   \n",
       "26   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.370207       2352   \n",
       "27   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.335058       2352   \n",
       "28   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.340769       2352   \n",
       "29   en_core_web_lg        [NOUN, ADJ, VERB, ADV]   3.313224       2352   \n",
       "\n",
       "          f1  precision    recall  accuracy   tp_rate   tn_rate   fp_rate  \\\n",
       "0   0.949318   0.941518  0.957248  0.911565  0.828231  0.083333  0.051446   \n",
       "1   0.945039   0.935484  0.954791  0.903912  0.826105  0.077806  0.056973   \n",
       "2   0.942962   0.939512  0.946437  0.900935  0.818878  0.082058  0.052721   \n",
       "3   0.946986   0.933206  0.961179  0.906888  0.831633  0.075255  0.059524   \n",
       "4   0.944539   0.931232  0.958231  0.902636  0.829082  0.073554  0.061224   \n",
       "5   0.946937   0.929891  0.964619  0.906463  0.834609  0.071854  0.062925   \n",
       "6   0.946522   0.912825  0.982801  0.903912  0.850340  0.053571  0.081207   \n",
       "7   0.947243   0.921077  0.974939  0.906037  0.843537  0.062500  0.072279   \n",
       "8   0.947772   0.924731  0.971990  0.907313  0.840986  0.066327  0.068452   \n",
       "9   0.949038   0.928941  0.970025  0.909864  0.839286  0.070578  0.064201   \n",
       "10  0.949071   0.932638  0.966093  0.910289  0.835884  0.074405  0.060374   \n",
       "11  0.946811   0.927830  0.966585  0.906037  0.836310  0.069728  0.065051   \n",
       "12  0.948675   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003   \n",
       "13  0.948675   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003   \n",
       "14  0.945480   0.909628  0.984275  0.901786  0.851616  0.050170  0.084609   \n",
       "15  0.946635   0.926588  0.967568  0.905612  0.837160  0.068452  0.066327   \n",
       "16  0.948580   0.945339  0.951843  0.910714  0.823554  0.087160  0.047619   \n",
       "17  0.951166   0.950000  0.952334  0.915391  0.823980  0.091412  0.043367   \n",
       "18  0.954245   0.950292  0.958231  0.920493  0.829082  0.091412  0.043367   \n",
       "19  0.952708   0.950147  0.955283  0.917942  0.826531  0.091412  0.043367   \n",
       "20  0.952033   0.943533  0.960688  0.916241  0.831207  0.085034  0.049745   \n",
       "21  0.949988   0.943314  0.956757  0.912840  0.827806  0.085034  0.049745   \n",
       "22  0.949951   0.939452  0.960688  0.912415  0.831207  0.081207  0.053571   \n",
       "23  0.947445   0.934097  0.961179  0.907738  0.831633  0.076105  0.058673   \n",
       "24  0.948184   0.934606  0.962162  0.909014  0.832483  0.076531  0.058248   \n",
       "25  0.946832   0.935701  0.958231  0.906888  0.829082  0.077806  0.056973   \n",
       "26  0.948489   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099   \n",
       "27  0.948489   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099   \n",
       "28  0.949602   0.932292  0.967568  0.911139  0.837160  0.073980  0.060799   \n",
       "29  0.950629   0.936576  0.965111  0.913265  0.835034  0.078231  0.056548   \n",
       "\n",
       "     fn_rate    eval_sec  \n",
       "0   0.036990    2.873141  \n",
       "1   0.039116    3.764681  \n",
       "2   0.046344    3.766310  \n",
       "3   0.033588    3.785317  \n",
       "4   0.036139    4.486029  \n",
       "5   0.030612    5.204129  \n",
       "6   0.014881    3.324612  \n",
       "7   0.021684    3.338723  \n",
       "8   0.024235    3.371420  \n",
       "9   0.025935    3.380885  \n",
       "10  0.029337    3.450147  \n",
       "11  0.028912    3.410430  \n",
       "12  0.020408    3.783208  \n",
       "13  0.020408    4.237439  \n",
       "14  0.013605    4.123538  \n",
       "15  0.028061    3.861520  \n",
       "16  0.041667  110.264073  \n",
       "17  0.041241  121.869415  \n",
       "18  0.036139  118.523124  \n",
       "19  0.038690  818.085331  \n",
       "20  0.034014   84.307367  \n",
       "21  0.037415  511.524469  \n",
       "22  0.034014   78.328459  \n",
       "23  0.033588   85.630378  \n",
       "24  0.032738   84.684338  \n",
       "25  0.036139  511.107338  \n",
       "26  0.031463   84.992992  \n",
       "27  0.031463   84.995357  \n",
       "28  0.028061   88.051997  \n",
       "29  0.030187   84.878537  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(model_results)\n",
    "df_results.drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f053b99",
   "metadata": {},
   "source": [
    "## Buscar mejor modelo\n",
    "Para el mejor modelo se seleccionó la métrica de F1, por lo que se ordenó el dataframe por F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e297a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>train_min</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>eval_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.642231</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.954245</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.920493</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>118.523124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>31.004590</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952708</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.955283</td>\n",
       "      <td>0.917942</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.038690</td>\n",
       "      <td>818.085331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.246148</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952033</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.916241</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>84.307367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.674562</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.951166</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.952334</td>\n",
       "      <td>0.915391</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.041241</td>\n",
       "      <td>121.869415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV]</td>\n",
       "      <td>3.313224</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>84.878537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>20.588683</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949988</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>0.956757</td>\n",
       "      <td>0.912840</td>\n",
       "      <td>0.827806</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>511.524469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.067748</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949951</td>\n",
       "      <td>0.939452</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.912415</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>78.328459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.340769</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.911139</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.073980</td>\n",
       "      <td>0.060799</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>88.051997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.093824</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.941518</td>\n",
       "      <td>0.957248</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.828231</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>2.873141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.166183</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949071</td>\n",
       "      <td>0.932638</td>\n",
       "      <td>0.966093</td>\n",
       "      <td>0.910289</td>\n",
       "      <td>0.835884</td>\n",
       "      <td>0.074405</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>3.450147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949038</td>\n",
       "      <td>0.928941</td>\n",
       "      <td>0.970025</td>\n",
       "      <td>0.909864</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.070578</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>3.380885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.235033</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>3.783208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.303449</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>4.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.302722</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948580</td>\n",
       "      <td>0.945339</td>\n",
       "      <td>0.951843</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.823554</td>\n",
       "      <td>0.087160</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>110.264073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.335058</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.995357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.370207</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.992992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.300843</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948184</td>\n",
       "      <td>0.934606</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>0.909014</td>\n",
       "      <td>0.832483</td>\n",
       "      <td>0.076531</td>\n",
       "      <td>0.058248</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>84.684338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.164007</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947772</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.971990</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.840986</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.024235</td>\n",
       "      <td>3.371420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>3.343620</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947445</td>\n",
       "      <td>0.934097</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.907738</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>85.630378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.159784</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947243</td>\n",
       "      <td>0.921077</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>3.338723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.205255</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946986</td>\n",
       "      <td>0.933206</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>3.785317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.422243</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946937</td>\n",
       "      <td>0.929891</td>\n",
       "      <td>0.964619</td>\n",
       "      <td>0.906463</td>\n",
       "      <td>0.834609</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>5.204129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>20.651870</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946832</td>\n",
       "      <td>0.935701</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>511.107338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.163125</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946811</td>\n",
       "      <td>0.927830</td>\n",
       "      <td>0.966585</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>3.410430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.236512</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946635</td>\n",
       "      <td>0.926588</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.905612</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>3.861520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.165792</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946522</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>3.324612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.301127</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945480</td>\n",
       "      <td>0.909628</td>\n",
       "      <td>0.984275</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.851616</td>\n",
       "      <td>0.050170</td>\n",
       "      <td>0.084609</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>4.123538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945039</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.954791</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.826105</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.039116</td>\n",
       "      <td>3.764681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.319370</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.944539</td>\n",
       "      <td>0.931232</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.902636</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.073554</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>4.486029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.204680</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.942962</td>\n",
       "      <td>0.939512</td>\n",
       "      <td>0.946437</td>\n",
       "      <td>0.900935</td>\n",
       "      <td>0.818878</td>\n",
       "      <td>0.082058</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>3.766310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       steps  ngrams  min_count  threshold  \\\n",
       "18                           [lemmatization]     NaN        NaN        NaN   \n",
       "19                           [lemmatization]     NaN        NaN        NaN   \n",
       "20         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "17                           [lemmatization]     NaN        NaN        NaN   \n",
       "29  [remove_stopwords, lemmatization, ngram]     2.0        7.0       25.0   \n",
       "21         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "22  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "28  [remove_stopwords, lemmatization, ngram]     3.0        7.0       25.0   \n",
       "0                         [remove_stopwords]     NaN        NaN        NaN   \n",
       "10                 [remove_stopwords, ngram]     2.0        7.0       30.0   \n",
       "9                  [remove_stopwords, ngram]     2.0        5.0       20.0   \n",
       "12                 [remove_stopwords, ngram]     3.0        5.0       20.0   \n",
       "13                 [remove_stopwords, ngram]     4.0        5.0       20.0   \n",
       "16                           [lemmatization]     NaN        NaN        NaN   \n",
       "27  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "26  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "24  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "8                  [remove_stopwords, ngram]     2.0        5.0       10.0   \n",
       "23  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "7                  [remove_stopwords, ngram]     2.0        3.0        5.0   \n",
       "3                                    [ngram]     2.0       10.0       50.0   \n",
       "5                                    [ngram]     4.0        7.0       20.0   \n",
       "25  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "11                 [remove_stopwords, ngram]     2.0       15.0       30.0   \n",
       "15                 [remove_stopwords, ngram]     3.0       15.0       30.0   \n",
       "6                  [remove_stopwords, ngram]     2.0        1.0        5.0   \n",
       "14                 [remove_stopwords, ngram]     4.0        2.0        5.0   \n",
       "1                                    [ngram]     2.0        7.0       20.0   \n",
       "4                                    [ngram]     3.0        7.0       20.0   \n",
       "2                                    [ngram]     2.0        5.0       10.0   \n",
       "\n",
       "          nlp_model               allowed_postags  train_min  evaluated  \\\n",
       "18   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   4.642231       2352   \n",
       "19  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  31.004590       2352   \n",
       "20   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.246148       2352   \n",
       "17   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   4.674562       2352   \n",
       "29   en_core_web_lg        [NOUN, ADJ, VERB, ADV]   3.313224       2352   \n",
       "21  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  20.588683       2352   \n",
       "22   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   3.067748       2352   \n",
       "28   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.340769       2352   \n",
       "0              None                          None   0.093824       2352   \n",
       "10             None                          None   0.166183       2352   \n",
       "9              None                          None   0.162186       2352   \n",
       "12             None                          None   0.235033       2352   \n",
       "13             None                          None   0.303449       2352   \n",
       "16   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   4.302722       2352   \n",
       "27   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.335058       2352   \n",
       "26   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.370207       2352   \n",
       "24   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   3.300843       2352   \n",
       "8              None                          None   0.164007       2352   \n",
       "23   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   3.343620       2352   \n",
       "7              None                          None   0.159784       2352   \n",
       "3              None                          None   0.205255       2352   \n",
       "5              None                          None   0.422243       2352   \n",
       "25  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]  20.651870       2352   \n",
       "11             None                          None   0.163125       2352   \n",
       "15             None                          None   0.236512       2352   \n",
       "6              None                          None   0.165792       2352   \n",
       "14             None                          None   0.301127       2352   \n",
       "1              None                          None   0.204473       2352   \n",
       "4              None                          None   0.319370       2352   \n",
       "2              None                          None   0.204680       2352   \n",
       "\n",
       "          f1  precision    recall  accuracy   tp_rate   tn_rate   fp_rate  \\\n",
       "18  0.954245   0.950292  0.958231  0.920493  0.829082  0.091412  0.043367   \n",
       "19  0.952708   0.950147  0.955283  0.917942  0.826531  0.091412  0.043367   \n",
       "20  0.952033   0.943533  0.960688  0.916241  0.831207  0.085034  0.049745   \n",
       "17  0.951166   0.950000  0.952334  0.915391  0.823980  0.091412  0.043367   \n",
       "29  0.950629   0.936576  0.965111  0.913265  0.835034  0.078231  0.056548   \n",
       "21  0.949988   0.943314  0.956757  0.912840  0.827806  0.085034  0.049745   \n",
       "22  0.949951   0.939452  0.960688  0.912415  0.831207  0.081207  0.053571   \n",
       "28  0.949602   0.932292  0.967568  0.911139  0.837160  0.073980  0.060799   \n",
       "0   0.949318   0.941518  0.957248  0.911565  0.828231  0.083333  0.051446   \n",
       "10  0.949071   0.932638  0.966093  0.910289  0.835884  0.074405  0.060374   \n",
       "9   0.949038   0.928941  0.970025  0.909864  0.839286  0.070578  0.064201   \n",
       "12  0.948675   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003   \n",
       "13  0.948675   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003   \n",
       "16  0.948580   0.945339  0.951843  0.910714  0.823554  0.087160  0.047619   \n",
       "27  0.948489   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099   \n",
       "26  0.948489   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099   \n",
       "24  0.948184   0.934606  0.962162  0.909014  0.832483  0.076531  0.058248   \n",
       "8   0.947772   0.924731  0.971990  0.907313  0.840986  0.066327  0.068452   \n",
       "23  0.947445   0.934097  0.961179  0.907738  0.831633  0.076105  0.058673   \n",
       "7   0.947243   0.921077  0.974939  0.906037  0.843537  0.062500  0.072279   \n",
       "3   0.946986   0.933206  0.961179  0.906888  0.831633  0.075255  0.059524   \n",
       "5   0.946937   0.929891  0.964619  0.906463  0.834609  0.071854  0.062925   \n",
       "25  0.946832   0.935701  0.958231  0.906888  0.829082  0.077806  0.056973   \n",
       "11  0.946811   0.927830  0.966585  0.906037  0.836310  0.069728  0.065051   \n",
       "15  0.946635   0.926588  0.967568  0.905612  0.837160  0.068452  0.066327   \n",
       "6   0.946522   0.912825  0.982801  0.903912  0.850340  0.053571  0.081207   \n",
       "14  0.945480   0.909628  0.984275  0.901786  0.851616  0.050170  0.084609   \n",
       "1   0.945039   0.935484  0.954791  0.903912  0.826105  0.077806  0.056973   \n",
       "4   0.944539   0.931232  0.958231  0.902636  0.829082  0.073554  0.061224   \n",
       "2   0.942962   0.939512  0.946437  0.900935  0.818878  0.082058  0.052721   \n",
       "\n",
       "     fn_rate    eval_sec  \n",
       "18  0.036139  118.523124  \n",
       "19  0.038690  818.085331  \n",
       "20  0.034014   84.307367  \n",
       "17  0.041241  121.869415  \n",
       "29  0.030187   84.878537  \n",
       "21  0.037415  511.524469  \n",
       "22  0.034014   78.328459  \n",
       "28  0.028061   88.051997  \n",
       "0   0.036990    2.873141  \n",
       "10  0.029337    3.450147  \n",
       "9   0.025935    3.380885  \n",
       "12  0.020408    3.783208  \n",
       "13  0.020408    4.237439  \n",
       "16  0.041667  110.264073  \n",
       "27  0.031463   84.995357  \n",
       "26  0.031463   84.992992  \n",
       "24  0.032738   84.684338  \n",
       "8   0.024235    3.371420  \n",
       "23  0.033588   85.630378  \n",
       "7   0.021684    3.338723  \n",
       "3   0.033588    3.785317  \n",
       "5   0.030612    5.204129  \n",
       "25  0.036139  511.107338  \n",
       "11  0.028912    3.410430  \n",
       "15  0.028061    3.861520  \n",
       "6   0.014881    3.324612  \n",
       "14  0.013605    4.123538  \n",
       "1   0.039116    3.764681  \n",
       "4   0.036139    4.486029  \n",
       "2   0.046344    3.766310  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.sort_values(by=['f1'], ascending=False)\n",
    "df_results.drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc387fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>train_min</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>eval_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>4.642231</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.954245</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.920493</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>118.523124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              steps  ngrams  min_count  threshold       nlp_model  \\\n",
       "18  [lemmatization]     NaN        NaN        NaN  en_core_web_lg   \n",
       "\n",
       "                 allowed_postags  train_min  evaluated        f1  precision  \\\n",
       "18  [NOUN, ADJ, VERB, ADV, PART]   4.642231       2352  0.954245   0.950292   \n",
       "\n",
       "      recall  accuracy   tp_rate   tn_rate   fp_rate   fn_rate    eval_sec  \n",
       "18  0.958231  0.920493  0.829082  0.091412  0.043367  0.036139  118.523124  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model with best F1\n",
    "df_results.head(1).drop(['model'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8278d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=df_results.iloc[0]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d6e78",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "Satinity check del mejor modelo. Se hace prueba con un comentario positivo y uno negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce1bbca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -20.99324952021314, 'probs': [-3.952062894899984, -9.552681513548059, -7.3589352005881645, -0.1295699111769321]}, 'NEG': {'prob': -19.400038181453695, 'probs': [-4.20068884579006, -6.825672448193656, -6.266056660258234, -2.1076202272117475]}, 'selected': 'NEG'}\n",
      ">> Selected class NEG\n"
     ]
    }
   ],
   "source": [
    "result = best_model.predict(\"the hotel was dirty and noisy\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4fd16a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POS': {'prob': -18.60460706303055, 'probs': [-3.952062894899984, -4.0390007830047745, -4.761291130350355, -5.722682343598503, -0.1295699111769321]}, 'NEG': {'prob': -23.9253527829405, 'probs': [-4.20068884579006, -4.52200098898739, -5.358499608352833, -7.736543112598472, -2.1076202272117475]}, 'selected': 'POS'}\n",
      ">> Selected class POS\n"
     ]
    }
   ],
   "source": [
    "result = best_model.predict(\"the hotel was very clean and I love it! :)\")\n",
    "print(result)\n",
    "print(f'>> Selected class {result[\"selected\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9cb93",
   "metadata": {},
   "source": [
    "## Métricas en dataframe de Test\n",
    "Una vez seleccionado el mejor modelo se calculan las métricas utilizando el dataframe de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17c7366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluados: 2353\n",
      "      TP Rate: 0.8266 (1945)\n",
      "      FP Rate: 0.0378 (89)\n",
      "      TN Rate: 0.0867 (204)\n",
      "      FN Rate: 0.0489 (115)\n",
      "    Accuracy: 0.9133\n",
      "    Precision: 0.9562\n",
      "    Recall: 0.9442\n",
      "    F1: 0.9502\n"
     ]
    }
   ],
   "source": [
    "# calculate test dataframe metrics with best model\n",
    "model_metrics = get_metrics(model=best_model, df=test)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833a2d2e",
   "metadata": {},
   "source": [
    "## Modelo Alternativo\n",
    "Como se puede observar en la tabla de resumen donde se encuentran todos los modelos, el modelo elegido utiliza el modelo de spacy en_core_web_lg y requiere más de 4.6 min para entrenar y más de 118 segundos para ejecutar los datos de validación (2352 datos). Considerando esto, si la aplicación requiriera que el entrenamiento e inferencia sean más rápidos se pudiera considerar el 3er mejor modelo, ya que entrena únicamente en 3.2 min y hace la inferencia de los casos de validación en 85 segundos (1.4x más rápido) y el performance en F1 disminuye únicamente por 0.0022 con la data de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6632f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alt=df_results.iloc[2]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a06723cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Evaluados: 2353\n",
      "      TP Rate: 0.8262 (1944)\n",
      "      FP Rate: 0.0387 (91)\n",
      "      TN Rate: 0.0858 (202)\n",
      "      FN Rate: 0.0493 (116)\n",
      "    Accuracy: 0.9120\n",
      "    Precision: 0.9553\n",
      "    Recall: 0.9437\n",
      "    F1: 0.9495\n"
     ]
    }
   ],
   "source": [
    "# calculate test dataframe metrics with best model\n",
    "model_metrics = get_metrics(model=model_alt, df=test)\n",
    "print_metrics(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed91d3a4",
   "metadata": {},
   "source": [
    "Como podemos observar, el F1 de este modelo en la data de test disminuyó únicamente en 0.0007 respecto al mejor modelo, pero tomando el 71% del tiempo en inferir los datos (29% de mejora respecto al mejor modelo), por lo que dependiendo de la aplicación también se podría utilizar este modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e695816",
   "metadata": {},
   "source": [
    "# Análisis de resultados\n",
    "De la tabla de resumen se pueden obtener las siguientes conclusiones:\n",
    "- Los modelos que lematizan (con cualquier modelo de spacy) son los que entregan mejor performance\n",
    "- Como se puede observar, los resultados de validation y test son bastante similares, lo que nos indica que el modelo generalizó bastante bien (no hizo overfitting)\n",
    "- El remover stopwords antes de lematizar se vio que hace que el entrenamiento y la inferencia sean más rápidos aunque disminuyen ligeramente el performance a pesar que técnicamente en el paso de lematizar también se eliminan stopwords al utilizar los allowed postags\n",
    "- El utilizar spacy y lematizar hace que el entrenamiento e inferencia sean lentos, por lo que si se requiere un modelo con el mejor tiempo de entrenamiento e inferencia se puede utilizar únicamente el remover stopwords (el 8vo mejor modelo), el cual obtuvo únicamente 0.0049 menos performance en F1 respecto al mejor modelo (F1=94.93%), pero con un tiempo de entrenamiento de 0.09min vs 4.64min y un tiempo de inferencia de 2.9s vs 118s, por lo que el performance no se degradaría tanto y el modelo sería mucho más rápido\n",
    "- El aplicar cualquier nivel de ngrams parecía disminuir performance de los modelos en general\n",
    "- Se realizaron pruebas con diferentes modelos de spacy y, como era de esperar, los modelos más pesados fueron los que mejor performance dieron (encore_web_trf > en_core_web_lg > en_core_web_md > en_core_web_sm) aunque el tiempo de entrenamiento e inferencia del trf fue bastante más lento que los otros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b36bd",
   "metadata": {},
   "source": [
    "# Guardar resultados\n",
    "Dado que el entrenamiento de los 30 modelos toma alrededor de 2h (tiempo medido en un servidor, no en computadora personal) se decidió guardar el resultado en un pickle para su uso posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8928d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_pickle('data/model_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43953901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_pickle('data/model_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e865e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steps</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>min_count</th>\n",
       "      <th>threshold</th>\n",
       "      <th>nlp_model</th>\n",
       "      <th>allowed_postags</th>\n",
       "      <th>model</th>\n",
       "      <th>train_min</th>\n",
       "      <th>evaluated</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>eval_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efc5c0030f0&gt;</td>\n",
       "      <td>4.642231</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.954245</td>\n",
       "      <td>0.950292</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.920493</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>118.523124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efc5b358c18&gt;</td>\n",
       "      <td>31.004590</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952708</td>\n",
       "      <td>0.950147</td>\n",
       "      <td>0.955283</td>\n",
       "      <td>0.917942</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.038690</td>\n",
       "      <td>818.085331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efc5a6acef0&gt;</td>\n",
       "      <td>3.246148</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.952033</td>\n",
       "      <td>0.943533</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.916241</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>84.307367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efc58716d30&gt;</td>\n",
       "      <td>4.674562</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.951166</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.952334</td>\n",
       "      <td>0.915391</td>\n",
       "      <td>0.823980</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.041241</td>\n",
       "      <td>121.869415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb868f2c88&gt;</td>\n",
       "      <td>3.313224</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.936576</td>\n",
       "      <td>0.965111</td>\n",
       "      <td>0.913265</td>\n",
       "      <td>0.835034</td>\n",
       "      <td>0.078231</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>84.878537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[remove_stopwords, lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb864a4b70&gt;</td>\n",
       "      <td>20.588683</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949988</td>\n",
       "      <td>0.943314</td>\n",
       "      <td>0.956757</td>\n",
       "      <td>0.912840</td>\n",
       "      <td>0.827806</td>\n",
       "      <td>0.085034</td>\n",
       "      <td>0.049745</td>\n",
       "      <td>0.037415</td>\n",
       "      <td>511.524469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb860a6940&gt;</td>\n",
       "      <td>3.067748</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949951</td>\n",
       "      <td>0.939452</td>\n",
       "      <td>0.960688</td>\n",
       "      <td>0.912415</td>\n",
       "      <td>0.831207</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>78.328459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb83dcc0f0&gt;</td>\n",
       "      <td>3.340769</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949602</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.911139</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.073980</td>\n",
       "      <td>0.060799</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>88.051997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[remove_stopwords]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb839a12b0&gt;</td>\n",
       "      <td>0.093824</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949318</td>\n",
       "      <td>0.941518</td>\n",
       "      <td>0.957248</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.828231</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.051446</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>2.873141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb83428438&gt;</td>\n",
       "      <td>0.166183</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949071</td>\n",
       "      <td>0.932638</td>\n",
       "      <td>0.966093</td>\n",
       "      <td>0.910289</td>\n",
       "      <td>0.835884</td>\n",
       "      <td>0.074405</td>\n",
       "      <td>0.060374</td>\n",
       "      <td>0.029337</td>\n",
       "      <td>3.450147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb82db1e10&gt;</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.949038</td>\n",
       "      <td>0.928941</td>\n",
       "      <td>0.970025</td>\n",
       "      <td>0.909864</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.070578</td>\n",
       "      <td>0.064201</td>\n",
       "      <td>0.025935</td>\n",
       "      <td>3.380885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb82689e10&gt;</td>\n",
       "      <td>0.235033</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>3.783208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7ca368d0&gt;</td>\n",
       "      <td>0.303449</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948675</td>\n",
       "      <td>0.922470</td>\n",
       "      <td>0.976413</td>\n",
       "      <td>0.908588</td>\n",
       "      <td>0.844813</td>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>4.237439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[lemmatization]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en_core_web_sm</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7c2a0cf8&gt;</td>\n",
       "      <td>4.302722</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948580</td>\n",
       "      <td>0.945339</td>\n",
       "      <td>0.951843</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.823554</td>\n",
       "      <td>0.087160</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>110.264073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7bd69fd0&gt;</td>\n",
       "      <td>3.335058</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.995357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7b8d71d0&gt;</td>\n",
       "      <td>3.370207</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948489</td>\n",
       "      <td>0.933810</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.909439</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>0.075680</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.031463</td>\n",
       "      <td>84.992992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_lg</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7b4ab390&gt;</td>\n",
       "      <td>3.300843</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.948184</td>\n",
       "      <td>0.934606</td>\n",
       "      <td>0.962162</td>\n",
       "      <td>0.909014</td>\n",
       "      <td>0.832483</td>\n",
       "      <td>0.076531</td>\n",
       "      <td>0.058248</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>84.684338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7b06d470&gt;</td>\n",
       "      <td>0.164007</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947772</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.971990</td>\n",
       "      <td>0.907313</td>\n",
       "      <td>0.840986</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.024235</td>\n",
       "      <td>3.371420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_md</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7aa15b38&gt;</td>\n",
       "      <td>3.343620</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947445</td>\n",
       "      <td>0.934097</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.907738</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.058673</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>85.630378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7a4f2eb8&gt;</td>\n",
       "      <td>0.159784</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.947243</td>\n",
       "      <td>0.921077</td>\n",
       "      <td>0.974939</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>3.338723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb79d91e48&gt;</td>\n",
       "      <td>0.205255</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946986</td>\n",
       "      <td>0.933206</td>\n",
       "      <td>0.961179</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.831633</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.033588</td>\n",
       "      <td>3.785317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb796a62b0&gt;</td>\n",
       "      <td>0.422243</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946937</td>\n",
       "      <td>0.929891</td>\n",
       "      <td>0.964619</td>\n",
       "      <td>0.906463</td>\n",
       "      <td>0.834609</td>\n",
       "      <td>0.071854</td>\n",
       "      <td>0.062925</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>5.204129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[remove_stopwords, lemmatization, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>en_core_web_trf</td>\n",
       "      <td>[NOUN, ADJ, VERB, ADV, PART]</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb78fa96d8&gt;</td>\n",
       "      <td>20.651870</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946832</td>\n",
       "      <td>0.935701</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.906888</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>511.107338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb78a5fb00&gt;</td>\n",
       "      <td>0.163125</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946811</td>\n",
       "      <td>0.927830</td>\n",
       "      <td>0.966585</td>\n",
       "      <td>0.906037</td>\n",
       "      <td>0.836310</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>3.410430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb7848c128&gt;</td>\n",
       "      <td>0.236512</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946635</td>\n",
       "      <td>0.926588</td>\n",
       "      <td>0.967568</td>\n",
       "      <td>0.905612</td>\n",
       "      <td>0.837160</td>\n",
       "      <td>0.068452</td>\n",
       "      <td>0.066327</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>3.861520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb5f5708d0&gt;</td>\n",
       "      <td>0.165792</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.946522</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.982801</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.850340</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>3.324612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[remove_stopwords, ngram]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb5a9eb668&gt;</td>\n",
       "      <td>0.301127</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945480</td>\n",
       "      <td>0.909628</td>\n",
       "      <td>0.984275</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.851616</td>\n",
       "      <td>0.050170</td>\n",
       "      <td>0.084609</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>4.123538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb59cd5f98&gt;</td>\n",
       "      <td>0.204473</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.945039</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>0.954791</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.826105</td>\n",
       "      <td>0.077806</td>\n",
       "      <td>0.056973</td>\n",
       "      <td>0.039116</td>\n",
       "      <td>3.764681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb5946ba58&gt;</td>\n",
       "      <td>0.319370</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.944539</td>\n",
       "      <td>0.931232</td>\n",
       "      <td>0.958231</td>\n",
       "      <td>0.902636</td>\n",
       "      <td>0.829082</td>\n",
       "      <td>0.073554</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.036139</td>\n",
       "      <td>4.486029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ngram]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;__main__.Model object at 0x7efb58dde9e8&gt;</td>\n",
       "      <td>0.204680</td>\n",
       "      <td>2352</td>\n",
       "      <td>0.942962</td>\n",
       "      <td>0.939512</td>\n",
       "      <td>0.946437</td>\n",
       "      <td>0.900935</td>\n",
       "      <td>0.818878</td>\n",
       "      <td>0.082058</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>3.766310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       steps  ngrams  min_count  threshold  \\\n",
       "18                           [lemmatization]     NaN        NaN        NaN   \n",
       "19                           [lemmatization]     NaN        NaN        NaN   \n",
       "20         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "17                           [lemmatization]     NaN        NaN        NaN   \n",
       "29  [remove_stopwords, lemmatization, ngram]     2.0        7.0       25.0   \n",
       "21         [remove_stopwords, lemmatization]     NaN        NaN        NaN   \n",
       "22  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "28  [remove_stopwords, lemmatization, ngram]     3.0        7.0       25.0   \n",
       "0                         [remove_stopwords]     NaN        NaN        NaN   \n",
       "10                 [remove_stopwords, ngram]     2.0        7.0       30.0   \n",
       "9                  [remove_stopwords, ngram]     2.0        5.0       20.0   \n",
       "12                 [remove_stopwords, ngram]     3.0        5.0       20.0   \n",
       "13                 [remove_stopwords, ngram]     4.0        5.0       20.0   \n",
       "16                           [lemmatization]     NaN        NaN        NaN   \n",
       "27  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "26  [remove_stopwords, lemmatization, ngram]     3.0        5.0       25.0   \n",
       "24  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "8                  [remove_stopwords, ngram]     2.0        5.0       10.0   \n",
       "23  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "7                  [remove_stopwords, ngram]     2.0        3.0        5.0   \n",
       "3                                    [ngram]     2.0       10.0       50.0   \n",
       "5                                    [ngram]     4.0        7.0       20.0   \n",
       "25  [remove_stopwords, lemmatization, ngram]     2.0        5.0       20.0   \n",
       "11                 [remove_stopwords, ngram]     2.0       15.0       30.0   \n",
       "15                 [remove_stopwords, ngram]     3.0       15.0       30.0   \n",
       "6                  [remove_stopwords, ngram]     2.0        1.0        5.0   \n",
       "14                 [remove_stopwords, ngram]     4.0        2.0        5.0   \n",
       "1                                    [ngram]     2.0        7.0       20.0   \n",
       "4                                    [ngram]     3.0        7.0       20.0   \n",
       "2                                    [ngram]     2.0        5.0       10.0   \n",
       "\n",
       "          nlp_model               allowed_postags  \\\n",
       "18   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "19  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "20   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "17   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "29   en_core_web_lg        [NOUN, ADJ, VERB, ADV]   \n",
       "21  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "22   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "28   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "0              None                          None   \n",
       "10             None                          None   \n",
       "9              None                          None   \n",
       "12             None                          None   \n",
       "13             None                          None   \n",
       "16   en_core_web_sm  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "27   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "26   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "24   en_core_web_lg  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "8              None                          None   \n",
       "23   en_core_web_md  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "7              None                          None   \n",
       "3              None                          None   \n",
       "5              None                          None   \n",
       "25  en_core_web_trf  [NOUN, ADJ, VERB, ADV, PART]   \n",
       "11             None                          None   \n",
       "15             None                          None   \n",
       "6              None                          None   \n",
       "14             None                          None   \n",
       "1              None                          None   \n",
       "4              None                          None   \n",
       "2              None                          None   \n",
       "\n",
       "                                        model  train_min  evaluated        f1  \\\n",
       "18  <__main__.Model object at 0x7efc5c0030f0>   4.642231       2352  0.954245   \n",
       "19  <__main__.Model object at 0x7efc5b358c18>  31.004590       2352  0.952708   \n",
       "20  <__main__.Model object at 0x7efc5a6acef0>   3.246148       2352  0.952033   \n",
       "17  <__main__.Model object at 0x7efc58716d30>   4.674562       2352  0.951166   \n",
       "29  <__main__.Model object at 0x7efb868f2c88>   3.313224       2352  0.950629   \n",
       "21  <__main__.Model object at 0x7efb864a4b70>  20.588683       2352  0.949988   \n",
       "22  <__main__.Model object at 0x7efb860a6940>   3.067748       2352  0.949951   \n",
       "28  <__main__.Model object at 0x7efb83dcc0f0>   3.340769       2352  0.949602   \n",
       "0   <__main__.Model object at 0x7efb839a12b0>   0.093824       2352  0.949318   \n",
       "10  <__main__.Model object at 0x7efb83428438>   0.166183       2352  0.949071   \n",
       "9   <__main__.Model object at 0x7efb82db1e10>   0.162186       2352  0.949038   \n",
       "12  <__main__.Model object at 0x7efb82689e10>   0.235033       2352  0.948675   \n",
       "13  <__main__.Model object at 0x7efb7ca368d0>   0.303449       2352  0.948675   \n",
       "16  <__main__.Model object at 0x7efb7c2a0cf8>   4.302722       2352  0.948580   \n",
       "27  <__main__.Model object at 0x7efb7bd69fd0>   3.335058       2352  0.948489   \n",
       "26  <__main__.Model object at 0x7efb7b8d71d0>   3.370207       2352  0.948489   \n",
       "24  <__main__.Model object at 0x7efb7b4ab390>   3.300843       2352  0.948184   \n",
       "8   <__main__.Model object at 0x7efb7b06d470>   0.164007       2352  0.947772   \n",
       "23  <__main__.Model object at 0x7efb7aa15b38>   3.343620       2352  0.947445   \n",
       "7   <__main__.Model object at 0x7efb7a4f2eb8>   0.159784       2352  0.947243   \n",
       "3   <__main__.Model object at 0x7efb79d91e48>   0.205255       2352  0.946986   \n",
       "5   <__main__.Model object at 0x7efb796a62b0>   0.422243       2352  0.946937   \n",
       "25  <__main__.Model object at 0x7efb78fa96d8>  20.651870       2352  0.946832   \n",
       "11  <__main__.Model object at 0x7efb78a5fb00>   0.163125       2352  0.946811   \n",
       "15  <__main__.Model object at 0x7efb7848c128>   0.236512       2352  0.946635   \n",
       "6   <__main__.Model object at 0x7efb5f5708d0>   0.165792       2352  0.946522   \n",
       "14  <__main__.Model object at 0x7efb5a9eb668>   0.301127       2352  0.945480   \n",
       "1   <__main__.Model object at 0x7efb59cd5f98>   0.204473       2352  0.945039   \n",
       "4   <__main__.Model object at 0x7efb5946ba58>   0.319370       2352  0.944539   \n",
       "2   <__main__.Model object at 0x7efb58dde9e8>   0.204680       2352  0.942962   \n",
       "\n",
       "    precision    recall  accuracy   tp_rate   tn_rate   fp_rate   fn_rate  \\\n",
       "18   0.950292  0.958231  0.920493  0.829082  0.091412  0.043367  0.036139   \n",
       "19   0.950147  0.955283  0.917942  0.826531  0.091412  0.043367  0.038690   \n",
       "20   0.943533  0.960688  0.916241  0.831207  0.085034  0.049745  0.034014   \n",
       "17   0.950000  0.952334  0.915391  0.823980  0.091412  0.043367  0.041241   \n",
       "29   0.936576  0.965111  0.913265  0.835034  0.078231  0.056548  0.030187   \n",
       "21   0.943314  0.956757  0.912840  0.827806  0.085034  0.049745  0.037415   \n",
       "22   0.939452  0.960688  0.912415  0.831207  0.081207  0.053571  0.034014   \n",
       "28   0.932292  0.967568  0.911139  0.837160  0.073980  0.060799  0.028061   \n",
       "0    0.941518  0.957248  0.911565  0.828231  0.083333  0.051446  0.036990   \n",
       "10   0.932638  0.966093  0.910289  0.835884  0.074405  0.060374  0.029337   \n",
       "9    0.928941  0.970025  0.909864  0.839286  0.070578  0.064201  0.025935   \n",
       "12   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003  0.020408   \n",
       "13   0.922470  0.976413  0.908588  0.844813  0.063776  0.071003  0.020408   \n",
       "16   0.945339  0.951843  0.910714  0.823554  0.087160  0.047619  0.041667   \n",
       "27   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099  0.031463   \n",
       "26   0.933810  0.963636  0.909439  0.833759  0.075680  0.059099  0.031463   \n",
       "24   0.934606  0.962162  0.909014  0.832483  0.076531  0.058248  0.032738   \n",
       "8    0.924731  0.971990  0.907313  0.840986  0.066327  0.068452  0.024235   \n",
       "23   0.934097  0.961179  0.907738  0.831633  0.076105  0.058673  0.033588   \n",
       "7    0.921077  0.974939  0.906037  0.843537  0.062500  0.072279  0.021684   \n",
       "3    0.933206  0.961179  0.906888  0.831633  0.075255  0.059524  0.033588   \n",
       "5    0.929891  0.964619  0.906463  0.834609  0.071854  0.062925  0.030612   \n",
       "25   0.935701  0.958231  0.906888  0.829082  0.077806  0.056973  0.036139   \n",
       "11   0.927830  0.966585  0.906037  0.836310  0.069728  0.065051  0.028912   \n",
       "15   0.926588  0.967568  0.905612  0.837160  0.068452  0.066327  0.028061   \n",
       "6    0.912825  0.982801  0.903912  0.850340  0.053571  0.081207  0.014881   \n",
       "14   0.909628  0.984275  0.901786  0.851616  0.050170  0.084609  0.013605   \n",
       "1    0.935484  0.954791  0.903912  0.826105  0.077806  0.056973  0.039116   \n",
       "4    0.931232  0.958231  0.902636  0.829082  0.073554  0.061224  0.036139   \n",
       "2    0.939512  0.946437  0.900935  0.818878  0.082058  0.052721  0.046344   \n",
       "\n",
       "      eval_sec  \n",
       "18  118.523124  \n",
       "19  818.085331  \n",
       "20   84.307367  \n",
       "17  121.869415  \n",
       "29   84.878537  \n",
       "21  511.524469  \n",
       "22   78.328459  \n",
       "28   88.051997  \n",
       "0     2.873141  \n",
       "10    3.450147  \n",
       "9     3.380885  \n",
       "12    3.783208  \n",
       "13    4.237439  \n",
       "16  110.264073  \n",
       "27   84.995357  \n",
       "26   84.992992  \n",
       "24   84.684338  \n",
       "8     3.371420  \n",
       "23   85.630378  \n",
       "7     3.338723  \n",
       "3     3.785317  \n",
       "5     5.204129  \n",
       "25  511.107338  \n",
       "11    3.410430  \n",
       "15    3.861520  \n",
       "6     3.324612  \n",
       "14    4.123538  \n",
       "1     3.764681  \n",
       "4     4.486029  \n",
       "2     3.766310  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada8567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1659c374",
   "metadata": {},
   "source": [
    "## Postags disponibles en spacy\n",
    "Estos son los postags que incluye spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895cfef",
   "metadata": {},
   "source": [
    "POS|DESCRIPTION|EXAMPLES\n",
    "---|---|---\n",
    "ADJ|adjective|*big, old, green, incomprehensible, first*\n",
    "ADP|adposition|*in, to, during*\n",
    "ADV|adverb|*very, tomorrow, down, where, there*\n",
    "AUX|auxiliary|*is, has (done), will (do), should (do)*\n",
    "CONJ|conjunction|*and, or, but*\n",
    "CCONJ|coordinating conjunction|*and, or, but*\n",
    "DET|determiner|*a, an, the*\n",
    "INTJ|interjection|*psst, ouch, bravo, hello*\n",
    "NOUN|noun|*girl, cat, tree, air, beauty*\n",
    "NUM|numeral|*1, 2017, one, seventy-seven, IV, MMXIV*\n",
    "PART|particle|*’s, not,*\n",
    "PRON|pronoun|*I, you, he, she, myself, themselves, somebody*\n",
    "PROPN|proper noun|*Mary, John, London, NATO, HBO*\n",
    "PUNCT|punctuation|*., (, ), ?*\n",
    "SCONJ|subordinating conjunction|*if, while, that*\n",
    "SYM|symbol|*$, %, §, ©, +, −, ×, ÷, =, :), *\n",
    "VERB|verb|*run, runs, running, eat, ate, eating*\n",
    "X|other|*sfpksdpsxmsa*\n",
    "SPACE|space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
